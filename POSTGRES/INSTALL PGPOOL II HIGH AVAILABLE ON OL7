+++++++++++++++++++++++++++++++++++++++++++++++++
+    INSTALL PGPOOL II HIGH AVAILABLE ON OL7    +
+++++++++++++++++++++++++++++++++++++++++++++++++

#CLUSTER SYSTEM CONFIGURATION
MACHINE             HOSTNAME            IP               
OL7_PGPOOL2_PRIM    pgpool2_prim        192.168.1.126   
OL7_PGPOOL2_STDB1   pgpool2_stdb1       192.168.1.127
OL7_PGPOOL2_STDB2   pgpool2_stdb2       192.168.1.128

#DISABLE SELINUX AND FIREWALLD

sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config && setenforce 0
systemctl stop firewalld && systemctl disable firewalld

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE STATIC NETWORK
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=no
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=enp0s3
UUID=49c279da-7065-40f5-b914-176973db2a0a
DEVICE=enp0s3
ONBOOT=yes
IPADDR=192.168.1.126
PREFIX=24
GATEWAY=192.168.1.1

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE HOSTNAME
hostnamectl 
hostnamectl set-hostname pgpool2_prim
hostnamectl --static

#REPEAT CONFIGURATION FOR OTHER MACHINES

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE /ETC/HOSTS
vi /etc/hosts
192.168.1.126   pgpool2_prim
192.168.1.127   pgpool2_stdb1
192.168.1.128   pgpool2_stdb2

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE DNS
vi /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE AND INSTALL SOFTWARE POSTGRESQL
vi /etc/yum.repos.d/oracle-linux-ol7.repo

[ol7_optional_developer]
name=Developer Preview of Oracle Linux $releasever Optional ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/optional/developer/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

[ol7_developer_EPEL]
name=Oracle Linux $releasever Development Packages ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/developer_EPEL/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

yum install openssl-devel libtermcap-devel readline-devel gcc bison flex perl libconfig-devel kernel-devel rsync -y
yum install zlib jzlib zlib-devel -y
yum install wget -y
yum install libmemcached libmemcached-devel memcached.x86_64 memcached-devel libpqxx -y 
systemctl enable memcached
systemctl start memcached

#REPEAT CONFIGURATION FOR OTHER MACHINES

#GET SFW DATABASE POSTGRES
wget https://ftp.postgresql.org/pub/source/$VERSION/postgresql-$VERSION.tar.gz
tar xvfz postgresql-$VERSION.tar.gz
cd postgresql-$VERSION

#INSTALL SFW DATABASE POSTGRES
## install in default path /usr/local/pgsql using port 5432
./configure --with-openssl
example [./configure --prefix=$path/pgsql-$VERSION --with-openssl]
make
make install
cd contrib/
make
make install
ls -l /usr/local/pgsql/

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CREATE USER AND DIRECTORY
adduser postgres
passwd postgres
mkdir -p /dados/data
mkdir -p /postgres/wals
chown postgres:postgres -R /dados
chown postgres:postgres -R /postgres

#REPEAT CONFIGURATION FOR OTHER MACHINES

su - postgres
/usr/local/pgsql-11.4/bin/initdb --locale=pt_BR.UTF-8 -D /dados/data/

#CONFIGURE ONLY ON PRIMARY

#CREATE SERVICE POSTGRESQL
vi /usr/lib/systemd/system/postgresql.service
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)

[Service]
Type=notify
User=postgres
ExecStart=/usr/local/pgsql-11.4/bin/postgres -D /dados/data
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutSec=0

[Install]
WantedBy=multi-user.target

systemctl daemon-reload
systemctl enable postgresql.service
systemctl status postgresql.service
systemctl start postgresql.service

#REPEAT CONFIGURATION FOR OTHER MACHINES

#INSTALL SFW DATABASE PGPOOL
yum install -y https://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-2.noarch.rpm
yum install -y pgpool-II-pg11*

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE POSTGRES PRIMARY
vi /dados/data/postgresql.conf
listen_addresses = '*'
port = 5432
wal_level = replica
archive_mode = on
archive_command = 'gzip -c %p > /postgres/wals/%f.gz'
archive_timeout = 600
max_wal_senders = 10
wal_keep_segments = 32
hot_standby = on
wal_log_hints = on 

systemctl stop postgresql.service
systemctl start postgresql.service

#CREATE USER FOR REPLICATION PRIMARY
su - postgtres
/usr/local/pgsql-11.4/bin/psql -U postgres -p 5432
SET password_encryption = 'scram-sha-256';
CREATE ROLE pgpool WITH LOGIN;
CREATE ROLE repl WITH REPLICATION LOGIN;
\password pgpool
\password repl
\password postgres
GRANT pg_monitor TO pgpool;
\q
exit

#CONFIGURE PG_HBA PRIMARY

vi /dados/data/pg_hba.conf
host    all             all             192.168.1.0/24          scram-sha-256
host    replication     all             192.168.1.0/24          scram-sha-256
su - postgres -c "/usr/local/pgsql-11.4/bin/pg_ctl reload -D /dados/data"

#TO PERFORM THE AUTOMATIVE FAILOVER IT IS NECESSARY TO EXCHANGE SSH KEYS USER ROOT AND POSTGRES USER
ssh-keygen -t rsa
cd .ssh/
ssh-copy-id -i id_rsa.pub root@192.168.1.126
ssh-copy-id -i id_rsa.pub root@192.168.1.127
ssh-copy-id -i id_rsa.pub root@192.168.1.128
ssh-copy-id -i id_rsa.pub postgres@192.168.1.126
ssh-copy-id -i id_rsa.pub postgres@192.168.1.127
ssh-copy-id -i id_rsa.pub postgres@192.168.1.128
su - postgres
ssh-keygen -t rsa
.ssh/
ssh-copy-id -i id_rsa.pub postgres@192.168.1.126
ssh-copy-id -i id_rsa.pub postgres@192.168.1.127
ssh-copy-id -i id_rsa.pub postgres@192.168.1.128

#CONFIGURE .PGPASS FILE
vi /dados/.pgpass
pgpool2_prim:5432:replication:repl:<repl user password>
pgpool2_stdb1:5432:replication:repl:<repl user passowrd>
pgpool2_stdb2:5432:replication:repl:<repl user passowrd>
pgpool2_prim:5432:postgres:postgres:<postgres user passowrd>
pgpool2_stdb1:5432:postgres:postgres:<postgres user passowrd>
pgpool2_stdb2:5432:postgres:postgres:<postgres user passowrd>
chmod 600 /dados/.pgpass

#REPEAT CONFIGURATION FOR OTHER MACHINES

#PGPOOL II CONFIGURATION FILES
cp -vf /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
vi /etc/pgpool-II/pgpool.conf
listen_addresses = '*'
sr_check_user = 'pgpool'
sr_check_password = '<pgpool password>'
health_check_period = 5
health_check_timeout = 30
health_check_user = 'pgpool'
health_check_password = '<pgpool password>'
health_check_max_retries = 3
backend_hostname0 = 'pgpool2_prim'
backend_port0 = 5432
backend_weight0 = 1
backend_data_directory0 = '/dados/data'
backend_flag0 = 'ALLOW_TO_FAILOVER'
backend_hostname1 = 'pgpool2_stdb1'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/dados/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'
backend_hostname2 = 'pgpool2_stdb2'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/dados/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
backend_application_name0 = 'pgpool2_prim'
backend_application_name1 = 'pgpool2_stdb1'
backend_application_name2 = 'pgpool2_stdb2'
failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %H %M %P %r %R'

#FAILOVER CONFIGURATION FILE
cp -vf /etc/pgpool-II/follow_master.sh.sample /etc/pgpool-II/follow_master.sh
cp -vf /etc/pgpool-II/failover.sh.sample /etc/pgpool-II/failover.sh
vi /etc/pgpool-II/failover.sh
#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"
OLD_PRIMARY_NODE_HOST="${11}"
OLD_PRIMARY_NODE_PORT="${12}"

PGHOME=/usr/local/pgsql-11.4


logger -i -p local1.info failover.sh: start: failed_node_id=$FAILED_NODE_ID old_primary_node_id=$OLD_PRIMARY_NODE_ID failed_host=$FAILED_NODE_HOST new_master_host=$NEW_MASTER_NODE_HOST

## If there's no master node anymore, skip failover.
if [ $NEW_MASTER_NODE_ID -lt 0 ]; then
    logger -i -p local1.info failover.sh: All nodes are down. Skipping failover.
        exit 0
fi

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info failover.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## If Standby node is down, skip failover.
if [ $FAILED_NODE_ID -ne $OLD_PRIMARY_NODE_ID ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$OLD_PRIMARY_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p $OLD_PRIMARY_NODE_PORT -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error failover.sh: drop replication slot "${FAILED_NODE_HOST}" failed
        exit 1
    fi

    exit 0
fi

## Promote Standby node.
logger -i -p local1.info failover.sh: Primary node is down, promote standby node ${NEW_MASTER_NODE_HOST}.

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote

if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: new_master_host=$NEW_MASTER_NODE_HOST promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

vi /etc/pgpool-II/follow_master.sh
#!/bin/bash
# This script is run after failover_command to synchronize the Standby with the new Primary.
# First try pg_rewind. If pg_rewind failed, use pg_basebackup.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/local/pgsql-11.4
ARCHIVEDIR=/postgres/wals
REPLUSER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898

logger -i -p local1.info follow_master.sh: start: Standby node ${FAILED_NODE_ID}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info follow_master.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`

if [ $PGVERSION -ge 12 ]; then
RECOVERYCONF=${FAILED_NODE_PGDATA}/myrecovery.conf
else
RECOVERYCONF=${FAILED_NODE_PGDATA}/recovery.conf
fi

## Check the status of Standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status


## If Standby is running, synchronize it with the new Primary.
if [ $? -eq 0 ]; then

    logger -i -p local1.info follow_master.sh: pg_rewind for $FAILED_NODE_ID

    # Create replication slot "${FAILED_NODE_HOST}"
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_create_physical_replication_slot('${FAILED_NODE_HOST}');\"
    "

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

        set -o errexit

        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        cat > ${RECOVERYCONF} << EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

        if [ ${PGVERSION} -ge 12 ]; then
            touch ${FAILED_NODE_PGDATA}/standby.signal
        else
            echo \"standby_mode = 'on'\" >> ${RECOVERYCONF}
        fi

        ${PGHOME}/bin/pg_rewind -D ${FAILED_NODE_PGDATA} --source-server=\"user=postgres host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT}\"

    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error follow_master.sh: end: pg_rewind failed. Try pg_basebackup.

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

            set -o errexit

            # Execute pg_basebackup
            rm -rf ${FAILED_NODE_PGDATA}
            rm -rf ${ARCHIVEDIR}/*
            ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U $REPLUSER -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream

            if [ ${PGVERSION} -ge 12 ]; then
                sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
                       -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${FAILED_NODE_PGDATA}/postgresql.conf
            fi

            cat > ${RECOVERYCONF} << EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

            if [ ${PGVERSION} -ge 12 ]; then
                touch ${FAILED_NODE_PGDATA}/standby.signal
            else
                echo \"standby_mode = 'on'\" >> ${RECOVERYCONF}
            fi
        "

        if [ $? -ne 0 ]; then
            # drop replication slot
            ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
                ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
            "

            logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
            exit 1
        fi
    fi

    # start Standby node on ${FAILED_NODE_HOST}
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start

    # If start Standby successfully, attach this node
    if [ $? -eq 0 ]; then

        # Run pcp_attact_node to attach Standby node to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U $PCP_USER -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [ $? -ne 0 ]; then
                logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
                exit 1
        fi

    # If start Standby failed, drop replication slot "${FAILED_NODE_HOST}"
    else

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool \
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c "SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')"

        logger -i -p local1.error follow_master.sh: end: follow master command failed
        exit 1
    fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master command complete
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------





















