####################
# POSTGRES INDEXES #
####################

Any question can be viewed from different points of view. 
We will talk about what an application developer using a DBMS should be interested in: what indexes exist, 
why there are so many different indexes in PostgreSQL, and how to use them to speed up queries. 
Perhaps, the topic could be revealed with fewer words, 
but we secretly hope for an inquisitive developer who is also interested in the details of the internal structure, 
especially since understanding such details allows not only listening to someone else's opinion, but also drawing our own conclusions.

The questions of developing new types of indexes will remain outside the discussion. 
This requires knowledge of the C language and belongs to the competence of a systems programmer rather than an application developer. 
For the same reason, we will practically not consider programming interfaces, but will focus only on what matters for using ready-to-use indexes.

In this part, we will talk about the division of responsibilities between the general indexing mechanism related to the database engine and the individual 
index access methods that can be added as extensions to PostgreSQL. In the next part, 
we will look at the accessor interface and important concepts such as classes and operator families. 
After such a long but necessary introduction, we will take a closer look at the structure and use of various types of indexes: 
Hash, B-tree, GiST, SP-GiST, GIN and RUM, BRIN and Bloom.

-----------
- Indexes -
-----------

Indexes in PostgreSQL are special database objects designed primarily to speed up data access. 
These are auxiliary structures: any index can be dropped and restored from the information in the table. 
Sometimes you hear that a DBMS can work without indexes, just slowly. However, 
this is not the case because indexes also serve to maintain some integrity constraints.

There are currently six different kinds of indexes built into PostgreSQL 9.6, and one more is available as an extension, 
made possible by important changes in version 9.6. So we should expect other types of indexes to appear in the near future.

Despite all the differences between the types of indexes (also called accessor methods), 
ultimately any of them establishes a correspondence between a key (for example, the value of an indexed column) and the table rows in which that key occurs. 
Lines are identified by a TID (tuple id), which consists of the file block number and the line position within the block. 
Then, knowing the key or some information about it, you can quickly read those lines in which the information of interest to us may be located, 
without looking through the entire table.

It is important to understand that the index, while speeding up data access, instead requires certain costs for its maintenance. 
For any operation on indexed data — whether it is inserting, deleting, or updating rows in a table — the indexes created on that table must be rebuilt, 
and within the same transaction. Note that updating table fields that have not been indexed does not rebuild the indexes; 
this mechanism is called HOT (Heap-Only Tuples).

Extensibility has several implications. To make the new access method easy to integrate into the system, 
PostgreSQL has a generic indexing mechanism. Its main task is to get the TID from the accessor and work with them:

  * reading data from corresponding versions of table rows;
  * sampling by a separate TID, or immediately by a set of TIDs (with building a bitmap);
  * checking the visibility of row versions for the current transaction, taking into account the isolation level.

The indexing engine is involved in the execution of queries; it is called according to the plan built during the optimization phase. 
The optimizer, when iterating and evaluating the various paths to execute a query, 
must understand the capabilities of all accessors that can potentially be applied. 
Will the access method be able to send data immediately in the desired order, 
or should we provide for sorting separately? is it possible to apply an accessor to search for null? - such questions are constantly solved by the optimizer.

Accessor information isn't just needed by the optimizer. When creating an index, 
the system needs to decide: can the index be built over multiple columns? can this index ensure uniqueness?

So, each access method must provide all the necessary information about itself. 
Before version 9.6, the pg_am table was used for this, and since 9.6 the data has moved deeper, inside special functions. 
We will get acquainted with this interface a little later.

The tasks of the accessor itself include everything else:

  * implementation of the index building algorithm and data paging (so that any index is processed in the same way by the buffer cache manager);
  * search for information in the index by the expression "indexed-field operator expression";
  * estimating the cost of using the index;
  * work with locks necessary for correct parallel execution of processes;
  * creation of a write-ahead log (WAL).

We'll first look at the capabilities of the general indexing mechanism, and then move on to looking at the various accessors.

-------------------
- Indexing Engine -
-------------------

The indexing mechanism allows PostgreSQL to work in the same way with a wide variety of accessors, given their capabilities.

Basic scanning methods

--------------
- Index scan -
--------------

You can work differently with the TIDs supplied by the index. Let's consider an example:

postgres = # create table t (a integer, b text, c boolean);
CREATE TABLE
postgres = # insert into t (a, b, c)
   select s.id, chr ((32 + random () * 94) :: integer), random () <0.01
   from generate_series (1,100000) as s (id)
   order by random ();
INSERT 0 100000
postgres = # create index on t (a);
CREATE INDEX
postgres = # analyze t;
ANALYZE

We have created a table with three fields. The first field contains numbers from 1 to 100000, 
and an index has been created on it (we don't care which one yet). The second field contains various ASCII characters besides non-printable ones. 
Finally, the third field contains a Boolean value that is true for about 1% of the rows and false for the rest. Rows are inserted into the table in random order.

Let's try to select a value according to the condition "a = 1". Note that the condition has the form "indexed-field operator expression", 
where the operator is "equal" and the expression (search key) is "1". In most cases, the condition must be exactly this kind for the index to be used.

postgres = # explain (costs off) select * from t where a = 1;
          QUERY PLAN
-------------------------------
 Index Scan using t_a_idx on t
   Index Cond: (a = 1)
(2 rows)

In this case, the optimizer has decided to use an Index Scan. When indexed, the accessor returns the TID values one at a time, 
until there are no matching rows. The indexing engine takes turns looking at the pages of the table pointed to by TIDs, 
getting the version of the row, checking its visibility in accordance with the multiversion rules, and returning the resulting data.

---------------
- Bitmap Scan -
---------------

Index scan works well when it comes to just a few values. However, as the sample grows, 
the chances of having to go back to the same table page multiple times increase. Therefore, in this case, the optimizer switches to a bitmap scan:

postgres = # explain (costs off) select * from t where a <= 100;
             QUERY PLAN
------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: (a <= 100)
   -> Bitmap Index Scan on t_a_idx
         Index Cond: (a <= 100)
(4 rows)

First, the accessor returns all TIDs that match the condition (the Bitmap Index Scan node), and a bitmap of row versions is built from them. 
The row versions are then read from the table (Bitmap Heap Scan) - each page will only be read once.

Note that in the second step, the condition can be rechecked (Recheck Cond). 
The sample may be too large for the row version bitmap to fit entirely into RAM (limited by the work_mem parameter). 
In this case, only a bitmap of pages containing at least one valid version of the string is built. Such a "rough" map takes up less space, 
but when reading a page, you have to double-check the conditions for each line stored there. Note that even in the case of a small sample (as in our example), 
the "Recheck Cond" step is still displayed in the plan, although it is not actually executed.

If conditions are imposed on multiple table fields and those fields are indexed, ,
a bitmap scan allows (if the optimizer deems it beneficial) multiple indexes to be used simultaneously. For each index, bitmaps of row versions are built, 
which are then bitwise logically multiplied (if the expressions are connected with the AND condition), 
or logically added (if the expressions are connected with the OR condition). For instance:

postgres = # create index on t (b);
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where a <= 100 and b = 'a';
                    QUERY PLAN
--------------------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: ((a <= 100) AND (b = 'a' :: text))
   -> BitmapAnd
         -> Bitmap Index Scan on t_a_idx
               Index Cond: (a <= 100)
         -> Bitmap Index Scan on t_b_idx
               Index Cond: (b = 'a' :: text)
(7 rows)

Here the BitmapAnd node concatenates the two bitmaps using the bitwise operation "and".

Bitmap scanning avoids repeated accesses to the same data page. 
But what if the data in the pages of the table is physically ordered in the same way as the index entries? Of course, 
you cannot completely rely on the physical order of the data in the pages - if you want sorted data, 
you must explicitly specify the ORDER BY clause in the query. But situations are quite possible in which, in fact, 
"almost all" of the data is ordered: for example, if rows are added in the desired order and do not change after that, 
or after the CLUSTER command has been executed. Then building a bitmap is an extra step, 
a regular index scan will be no worse (if you do not take into account the possibility of combining several indices). 
Therefore, when choosing an access method, the planner looks into special statistics that show the degree of data ordering:

postgres = # select attname, correlation from pg_stats where tablename = 't';
 attname  | correlation
--------- + -------------
 b        | 0.533512
 c        | 0.942365
 a        | -0.00768816
(3 rows)

Values close in absolute value to one indicate a high ordering (as for column c), and close to zero, on the contrary, 
indicate a chaotic distribution (column a).

-------------------
- Sequential scan -
-------------------

For the sake of completeness, it should be said that under the non-selective condition, 
the optimizer will prefer to use the index to scan the entire table sequentially:

postgres = # explain (costs off) select * from t where a <= 40000;
       QUERY PLAN
------------------------
 Seq Scan on t
   Filter: (a <= 40000)
(2 rows)

And he will be right. The point is that indexes work better, the higher the selectivity of the condition, that is, the fewer rows that satisfy it. 
As the sample grows, so does the overhead of reading the index pages.

The situation is aggravated by the fact that sequential reading is faster than reading pages "out of order". 
This is especially true for hard drives, where the mechanical operation of bringing the head to the track takes significantly longer than reading the data itself;
this effect is less pronounced with SSDs. To take into account the difference in the cost of access, 
there are two parameters seq_page_cost and random_page_cost, which can be set not only globally, but also at the table space level, 
thus taking into account the characteristics of different disk subsystems.

--------------------
- Covering indices -
--------------------

Typically, the main purpose of an accessor is to return matching table row identifiers so that the indexing engine can read the data it needs. 
But what if the index already contains all the data it needs to query? Such an index is called covering, 
in which case the optimizer can use an Index Only Scan:

postgres = # vacuum t;
VACUUM
postgres = # explain (costs off) select a from t where a <100;
             QUERY PLAN
------------------------------------
 Index Only Scan using t_a_idx on t
   Index Cond: (a <100)
(2 rows)

The name might suggest that the indexing mechanism does not access the table at all, getting all the information it needs solely from the accessor. 
This is not entirely true, because indexes in PostgreSQL do not contain information to judge the visibility of rows. 
Therefore, the accessor returns all the row versions that match the search condition, whether they are visible to the current transaction or not.

However, if the indexing engine had to look at the table each time to determine visibility, this scan method would be no different from a regular index scan.

The problem is solved by the fact that PostgreSQL maintains a so-called visibility map for tables, 
in which the vacuum process marks pages in which data has not changed long enough for all transactions to see them, 
regardless of the start time and isolation level. If the identifier of the row returned by the index refers to such a page, then visibility can be omitted.

Therefore, regular cleaning improves the performance of the covering indexes. Moreover, 
the optimizer takes into account the number of uncleaned rows and may opt out of using an index-only scan if it predicts large visibility overhead.

The number of table accesses that were forced can be found using the explain analyze command:

postgres = # explain (analyze, costs off) select a from t where a <100;
                                  QUERY PLAN
-------------------------------------------------- -----------------------------
 Index Only Scan using t_a_idx on t (actual time = 0.025..0.036 rows = 99 loops = 1)
   Index Cond: (a <100)
   Heap Fetches: 0
 Planning time: 0.092 ms
 Execution time: 0.059 ms
(5 rows)

In this case, it was not necessary to access the table (Heap Fetches: 0), since cleanup has just been performed. 
In general, the closer this number is to zero, the better.

Not all indexes store the indexed values themselves along with row identifiers. If an accessor cannot return data, it cannot be used for index-only scans.

--------
- Null -
--------

Undefined values play an important role in relational databases as a convenient way to represent the fact that a value does not exist or is not known.

But special importance also requires a special attitude to itself. Normal Boolean logic turns into three-valued logic; 
it is not clear whether the undefined value should be less than normal values or more (hence the special constructions for sorting NULLS FIRST and NULLS LAST); 
it is not obvious whether it is necessary to take into account undefined values in aggregate functions or not; special statistics required for the scheduler ...

From the point of view of indexing support with undefined values, there is also ambiguity: should such values be indexed or not? If you do not index null, 
the index can be more compact. But if you index, then it becomes possible to use the index for conditions of the form "indexed-field IS [NOT] NULL", 
as well as as a covering index in the absence of conditions on the table (since in this case the index must return the data of all rows of the table, 
including number and with undefined values).

For each accessor, its developers make their own decision whether to index nulls or not. But, as a rule, they are still indexed.

--------------------------
- Multiple field indexes -
--------------------------

Conditions on multiple fields can be supported using multi-column indexes. For example, we could create an index on two fields of our table:

postgres = # create index on t (a, b);
CREATE INDEX
postgres = # analyze t;
ANALYZE

The optimizer will most likely prefer this index over bitmap concatenation, since here we immediately get the desired TIDs without any additional actions:

postgres = # explain (costs off) select * from t where a <= 100 and b = 'a';
                   QUERY PLAN
------------------------------------------------
 Index Scan using t_a_b_idx on t
   Index Cond: ((a <= 100) AND (b = 'a' :: text))
(2 rows)

A multi-column index can also be used to speed up selection by condition for some of the fields - starting from the first:

postgres = # explain (costs off) select * from t where a <= 100;
              QUERY PLAN
--------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: (a <= 100)
   -> Bitmap Index Scan on t_a_b_idx
         Index Cond: (a <= 100)
(4 rows)

Typically, if no condition is imposed on the first field, the index will not be used. However, in some cases, 
the optimizer may find it more beneficial than sequential scans. We'll cover this topic in more detail when we look at btree indexes.

Not all accessors support creating indexes on multiple columns.

----------------------
- Expression indexes -
----------------------

We talked about the search condition being "indexed-field operator expression". In the example below, 
the index will not be used because an expression with it is used instead of the field name:

postgres = # explain (costs off) select * from t where lower (b) = 'a';
                QUERY PLAN
------------------------------------------
 Seq Scan on t
   Filter: (lower ((b) :: text) = 'a' :: text)
(2 rows)

This particular query can be easily rewritten so that only the field name appears to the left of the operator. 
But if this is not possible, expression indexes (functional indexes) come to the rescue:

postgres = # create index on t (lower (b));
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where lower (b) = 'a';
                     QUERY PLAN
-------------------------------------------------- -
 Bitmap Heap Scan on t
   Recheck Cond: (lower ((b) :: text) = 'a' :: text)
   -> Bitmap Index Scan on t_lower_idx
         Index Cond: (lower ((b) :: text) = 'a' :: text)
(4 rows)

A functional index is created not by a table field, but by an arbitrary expression; 
the optimizer will take such an index into account for conditions of the form "indexed-expression operator expression". 
If the calculation of the indexed expression is a costly operation, then updating the index will also require significant computing resources.

It should also be borne in mind that separate statistics are collected for the indexed expression. It can be seen in the pg_stats view by the index name:

postgres = # \ d t
       Table "public.t"
 Column | Type | Modifiers
-------- + --------- + -----------
 a | integer |
 b | text |
 c | boolean |
Indexes:
    "t_a_b_idx" btree (a, b)
    "t_a_idx" btree (a)
    "t_b_idx" btree (b)
    "t_lower_idx" btree (lower (b))

postgres = # select * from pg_stats where tablename = 't_lower_idx';
...

If necessary, you can control the number of histogram buckets in the same way as for regular table fields (bearing in mind that the column name 
can be different depending on the indexed expression):

postgres=# \d t_lower_idx
 Index "public.t_lower_idx"
 Column | Type | Definition
--------+------+------------
 lower  | text | lower(b)
btree, for table "public.t"

postgres=# alter index t_lower_idx alter column "lower" set statistics 69;
ALTER INDEX

-------------------
- Partial Indexes -
-------------------

Sometimes it becomes necessary to index only part of the table rows. 
This is usually due to a strong uneven distribution: a rare value makes sense to search by index, 
but a frequent value is easier to find by a full scan of the table.

Of course, you can build a regular index on column "c" and it will work as we expect:

postgres = # create index on t (c);
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where c;
          QUERY PLAN
-------------------------------
 Index Scan using t_c_idx on t
   Index Cond: (c = true)
   Filter: c
(3 rows)

postgres = # explain (costs off) select * from t where not c;
    QUERY PLAN
-------------------
 Seq Scan on t
   Filter: (NOT c)
(2 rows)

Moreover, the index is 276 pages:

postgres = # select relpages from pg_class where relname = 't_c_idx';
 relpages
----------
      276
(1 row)

But since column "c" is true for only one percent of the rows, 99% of the index is simply never used. In this case, a partial index can be built:

postgres = # create index on t (c) where c;
CREATE INDEX
postgres = # analyze t;
ANALYZE

The size of such an index was reduced to 5 pages:

postgres = # select relpages from pg_class where relname = 't_c_idx1';
 relpages
----------
     five
(1 row)

In some cases, the difference in volume and performance can be quite significant.

-----------
- Sorting -
-----------

If the accessor returns row ids in sort order, this gives the optimizer additional options for executing the query.

You can scan the table and then sort the data:

postgres = # set enable_indexscan = off;
SET
postgres = # explain (costs off) select * from t order by a;
     QUERY PLAN
---------------------
 Sort
   Sort Key: a
   -> Seq Scan on t
(3 rows)

And you can read the data using the index immediately in the sort order:

postgres = # set enable_indexscan = on;
SET
postgres = # explain (costs off) select * from t order by a;
          QUERY PLAN
-------------------------------
 Index Scan using t_a_idx on t
(1 row)

Of all the accessors, only btree can return sorted data, so let's postpone a more detailed discussion until we look at this type of index.

-------------------------
- Parallel construction -
-------------------------

Typically building an index requires a SHARE lock on the table. This lock allows you to read data from the table, 
but disallows any changes while the index is being built.

You can verify this if, at the time of creating the index, say, on table t, in another session, execute the query:

postgres = # select mode, granted from pg_locks where relation = 't' :: regclass;
   mode     | granted
----------- + ---------
 ShareLock  | t
(1 row)

If the table is large enough and is actively used in insert, update, or delete mode, 
this may be invalid - modifying sessions will wait a long time to release the lock.

In this case, you can use parallel index creation:

postgres = # create index concurrently on t (a);
CREATE INDEX

Such a command sets a SHARE UPDATE EXCLUSIVE lock, which allows both reading and modifying data (only changing the structure of the table is prohibited, 
as well as simultaneously performing cleanup, analysis, or building another index on the same table).

However, there is a downside. First, the index will build more slowly than usual, since instead of one pass through the table, two are performed, 
and you still need to wait for the completion of parallel transactions that modify the data.

Second, building an index in parallel can result in a deadlock or unique constraint violation. The index is nevertheless created, 
but in a "non-working" state; in this case, it must be deleted and recreated again. 
Broken indexes are marked with the word INVALID in the output of the psql \ d command, and the complete list can be obtained with the query:

postgres = # select indexrelid :: regclass index_name, indrelid :: regclass table_name from pg_index where not indisvalid;
 index_name  | table_name
------------ + ------------
 t_a_idx     | t
(1 row)

--------------
- Properties -
--------------

All properties of access methods are presented in the pg_am table (am - access method). From this table, you can get the list of available methods itself:

postgres = # select amname from pg_am;
amname
--------
btree
hash
gist
gin
spgist
brin
(6 rows)

Although sequential scanning can rightfully be classified as access method, historically it has not been included in this list.

In PostgreSQL 9.5 and older, each property was represented by a separate field in the pg_am table. Since version 9.6, 
properties are polled by special functions and are divided into several levels:

accessor properties - pg_indexam_has_property,
properties of a particular index - pg_index_has_property,
properties of the individual columns of the index - pg_index_column_has_property.

The division into accessor and index levels is made for the future: at present, 
all indexes created based on the same accessor will always have the same properties.


Accessor properties include the following four (for example, btree):

postgres = # select a.amname, p.name, pg_indexam_has_property (a.oid, p.name)
from pg_am a,
unnest (array ['can_order', 'can_unique', 'can_multi_col', 'can_exclude']) p (name)
where a.amname = 'btree' order by a.amname;
amname   | name            | pg_indexam_has_property
-------- + --------------- + -------------------------
btree    | can_order       | t
btree    | can_unique      | t
btree    | can_multi_col   | t
btree    | can_exclude     | t
(4 rows)

 * can_order 
   The accessor allows you to specify the sort order of values when the index is created (currently only applicable for btree);
 * can_unique
   Support for unique constraint and primary key (only applicable for btree);
 * can_multi_col
   An index can be built on multiple columns;
 * can_exclude
   Support for limiting the EXCLUDE exception.

Index related properties (let's take an existing one for example):

postgres = # select p.name, pg_index_has_property ('t_a_idx' :: regclass, p.name)
from unnest (array ['clusterable', 'index_scan', 'bitmap_scan', 'backward_scan']) p (name);
name            | pg_index_has_property
--------------- + -----------------------
clusterable     | t
index_scan      | t
bitmap_scan     | t
backward_scan   | t
(4 rows)

 * clusterable
   Possibility of reordering table rows in accordance with the given index (clustering by the CLUSTER command of the same name);
 * index_scan
   Index scan support. This property may seem strange, but not all indexes can return TIDs one at a time - some give all results at once and only support bitmap scanning;
 * bitmap_scan
   Bitmap scanning support;
 * backward_scan
   Returns the result in the reverse order of that specified when the index was created.

Finally, the column properties:

postgres = # select p.name, pg_index_column_has_property ('t_a_idx' :: regclass, 1, p.name)
from unnest (array ['asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls']) p (name);
name                 | pg_index_column_has_property
-------------------- + ----------------------------- -
asc                  | t
desc                 | f
nulls_first          | f
nulls_last           | t
orderable            | t
distance_orderable   | f
returnable           | t
search_array         | t
search_nulls         | t
(9 rows)

 * asc, desc, nulls_first, nulls_last, orderable
   These properties are related to the ordering of values ​​(we'll talk about them when we get to btree indexes);
 * distance_orderable
   Returns the result in the sort order by operation (currently only applicable for gist and rum indexes);
 * returnable
   The ability to use an index without accessing the table, that is, support for only index access;
 * search_array
   Support for searching multiple values for the "indexed-field IN (list_constants)" or, equivalently, "indexed-field = ANY (array_constants)";
 * search_nulls
   The ability to search for is null and is not null conditions.

We have already discussed some of the properties in detail earlier. Some of the properties are currently implemented by only one method. 
We will consider such possibilities when we talk about this particular method.

---------------------------------
- Operator Classes and Families -
---------------------------------

In addition to the set of "skills", you also need to know what types of data and with what operators the access method works. 
For this, PostgreSQL has the concepts of an operator class and an operator family.

The operator class contains a minimal set of operators (and possibly helper functions) for working with an index on some data type.

A class is always part of a family of operators. In this case, several classes can be included in one common family if they have the same semantics. 
For example, the integer_ops family includes classes int8_ops, int4_ops, and int2_ops for different sized but identical bigint, integer, and smallint types:

postgres = # select opfname, opcname, opcintype :: regtype
from pg_opclass opc, pg_opfamily opf
where opf.opfname = 'integer_ops'
and opc.opcfamily = opf.oid
and opf.opfmethod = (select oid from pg_am where amname = 'btree');
opfname       | opcname    | opcintype
------------- + ---------- + -----------
integer_ops   | int2_ops   | smallint
integer_ops   | int4_ops   | integer
integer_ops   | int8_ops   | bigint
(3 rows)

Another example: the datetime_ops family includes operator classes for working with dates (both without time, and with time):

postgres = # select opfname, opcname, opcintype :: regtype
from pg_opclass opc, pg_opfamily opf
where opf.opfname = 'datetime_ops'
and opc.opcfamily = opf.oid
and opf.opfmethod = (select oid from pg_am where amname = 'btree');
opfname        | opcname           | opcintype
-------------- + ----------------- + ----------------- ------------
datetime_ops   | date_ops          | date
datetime_ops   | timestamptz_ops   | timestamp with time zone
datetime_ops   | timestamp_ops     | timestamp without time zone
(3 rows)

The family can also include additional operators for comparing values of different types. 
By grouping into families, the planner can use an index for predicates with values of different types. The family may also contain other helper functions.

In most cases, you don't need to know anything about operator families and classes. 
Usually we just create an index and use some default operator class.

However, you can specify the operator class explicitly. A simple example when needed: in a database with a non-C collation, 
a normal index on a text field does not support LIKE:

postgres = # show lc_collate;
lc_collate
-------------
en_US.UTF-8
(1 row)
postgres = # explain (costs off) select * from t where b like 'A%';
QUERY PLAN
-----------------------------
Seq Scan on t
Filter: (b ~~ 'A%' :: text)
(2 rows)

You can overcome this limitation by creating an index with the operator class text_pattern_ops (note how the condition has changed in the plan):

postgres = # create index on t (b text_pattern_ops);
CREATE INDEX
postgres = # explain (costs off) select * from t where b like 'A%';
QUERY PLAN
-------------------------------------------------- --------------
Bitmap Heap Scan on t
Filter: (b ~~ 'A%' :: text)
-> Bitmap Index Scan on t_b_idx1
Index Cond: ((b ~> = ~ 'A' :: text) AND (b ~ <~ 'B' :: text))
(4 rows)

--------------------
- System directory -
--------------------

To conclude this part, we present a small diagram of the system catalog tables that directly relate to operator classes and families.
https://mega.nz/file/xo0l1K5I#0JNL6ibUVgBLSNxvG7F0JKULOGHB6YsYlrfr1EXD-yU

All these tables are, of course, detailed.
https://www.postgresql.org/docs/13/catalogs.html

Using the system catalog, you can find the answer to a number of questions without even looking at the documentation. 
For example, what types of data can such and such an accessor work with?

postgres = # select opcname, opcintype :: regtype
from pg_opclass
where opcmethod = (select oid from pg_am where amname = 'btree')
order by opcintype :: regtype :: text;
opcname               | opcintype
--------------------- + ---------------------------- -
abstime_ops           | abstime
array_ops             | anyarray
enum_ops              | anyenum
...

What operators are included in the class (and therefore the index can be used for conditional access that includes such an operator)?

postgres = # select amop.amopopr :: regoperator
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'array_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'btree'
and amop.amoplefttype = opc.opcintype;
amopopr
-----------------------
<(anyarray, anyarray)
<= (anyarray, anyarray)
= (anyarray, anyarray)
> = (anyarray, anyarray)
> (anyarray, anyarray)
(5 rows)

---------------
- Index Types -
---------------

--------
- Hash -
--------

General theory

Many modern programming languages include hash tables as their underlying data type. Outwardly, it looks like a regular array, but not an integer, 
but any data type (for example, a string) is used as an index. The hash index is similar in PostgreSQL. How it works?

Typically, data types have very large ranges of valid values: how many different rows can theoretically be represented in a column of type text? 
At the same time, how many different values are actually stored in the text column of some table? Usually not much.

The idea of hashing is to associate a small number with a value of any data type (from 0 to N − 1, N values in total). 
This mapping is called a hash function. The resulting number can be used as an index of a regular array, where you can add references to table rows (TID). 
The elements of such an array are called hash table buckets - several TIDs can be in one bucket if the same indexed value occurs in different rows.

The hash function is better, the more evenly it distributes the original values across the buckets. 
But even a good function will sometimes give the same result for different input values - this is called collision. 
So, one bucket may contain TIDs corresponding to different keys, and therefore the TIDs obtained from the index must be rechecked.

Just as an example: what hash function can you think of for strings? Let the number of baskets be 256. 
Then the code of the first character can be taken as the basket number (for example, we have a one-byte encoding). 
Is this a good hash function? Obviously not: if all lines start with the same character, they all end up in the same bucket; 
there is no question of uniformity, you will have to recheck all the values ​​and the whole meaning of hashing will be lost.
What if you sum the codes of all characters modulo 256? It will be much better, although not perfect either. 
If you are wondering how such a hash function actually works in PostgreSQL, see the definition of hash_any () in hashfunc.c.

Index device

Let's go back to the hash index. Our task is to quickly find the corresponding TID by the value of some data type (indexing key).

When inserting into the index, calculate the hash function for the key. Hash functions in PostgreSQL always return integer, 
which corresponds to a range of 232 ≈ 4 billion values. The number of buckets is initially equal to two and increases dynamically, 
adjusting to the amount of data; the bucket number can be calculated from the hash code using bit arithmetic. Let's put our TID in this basket.

But this is not enough, because TIDs corresponding to different keys can get into one basket. 
How to be? It would be possible to write the original key value along with the TID to the bucket, but this would greatly increase the size of the index. 
So to save space, not the key itself is saved in the basket, but its hash code.

When searching the index, we compute the hash function for the key and get the bucket number. 
All that remains is to iterate through the entire contents of the basket and return only the matching TIDs with the required hash codes. 
This is done efficiently because hash-TID pairs are stored in order.

But it may so happen that two different keys will not just end up in the same bucket, 
but will also have the same 4-byte hash codes - no one canceled collisions. Therefore, 
the access method asks the general indexing mechanism to control each TID, 
rechecking the condition against the table row (the mechanism can do this along with the visibility check).

Page organization

If you look at the index not from the point of view of scheduling and executing a query, but through the eyes of a buffer cache manager, 
it turns out that all information, all index records must be packed into pages. 
Such index pages are buffered and flushed out in the same way as table pages.

https://mega.nz/file/V99GSa5b#imu61Yt23noh2sKw8AzJI_9KpJKLOrkFsySkLRCWXCw

The hash index, as you can see in the picture, uses pages (gray rectangles) of four types:

 * Meta page - zero page, contains information about what is inside the index;
 * Bucket pages - the main pages of the index, store data in the form of pairs "hash code - TID";
 * Overflow pages - are arranged in the same way as cart pages and are used when one page is not enough for the cart;
 * Bitmap pages - These are the overflow pages that have been vacated and can be used for other buckets.
 
Down arrows from index page items symbolize TIDs - links to table rows.
With the next increase in the index, twice as many buckets (and, accordingly, pages) are created at one time than the last time. 
In order not to select at once such a potentially large number of pages, in version 10 they made a smoother increase in size. Well, 
overflow pages are just allocated as needed and tracked in bitmap pages, which are also allocated as needed.
Note that the hash index cannot be reduced in size. If you delete some of the indexed rows, 
the once allocated pages are no longer returned to the operating system, but only reused for new data after cleaning (VACUUM). 
The only way to reduce the physical size of the index is to rebuild it from scratch with the REINDEX or VACUUM FULL command. 

Example

Let's give an example of creating a hash index. In order not to invent our own tables, 
here and further we will use the demo database on air transportation, and this time we will take the table of flights.

demo = # create index on flights using hash (flight_no);
WARNING: hash indexes are not WAL-logged and their use is discouraged
CREATE INDEX

demo = # explain (costs off) select * from flights where flight_no = 'PG0001';
QUERY PLAN
-------------------------------------------------- -
Bitmap Heap Scan on flights
Recheck Cond: (flight_no = 'PG0001' :: bpchar)
-> Bitmap Index Scan on flights_flight_no_idx
Index Cond: (flight_no = 'PG0001' :: bpchar)
(4 rows)

An unpleasant feature of the current implementation of a hash index is that actions with it are not written to the write-ahead log 
(which is what PostgreSQL warns us about when creating an index). As a consequence, 
hash indexes cannot be recovered from a failure and do not participate in replication. In addition, 
the hash index is significantly inferior to the B-tree in terms of versatility, and its effectiveness also raises questions. 
That is, there is no practical sense in using such indexes now.

However, the situation will change this fall with the release of the tenth version of PostgreSQL. In it, 
the hash index was finally provided with log support and additionally optimized memory allocation and the efficiency of concurrent work.

Hashing semantics

Why has the hash index survived almost from the very birth of PostgreSQL to the present day in a state in which it cannot be used? 
The fact is that the hashing algorithm is used very widely in the DBMS (in particular, for hash joins and groupings), 
and the system needs to know which hash function to apply to which data types. But this mapping is not static, 
it cannot be set once and for all - PostgreSQL allows you to add new types on the fly. Here in the hash access method such a match is contained, 
and it is presented in the form of binding auxiliary functions to operator families:

demo = # select opf.opfname as opfamily_name,
amproc.amproc :: regproc AS opfamily_procedure
from pg_am am,
pg_opfamily opf,
pg_amproc amproc
where opf.opfmethod = am.oid
and amproc.amprocfamily = opf.oid
and am.amname = 'hash'
order by opfamily_name,
opfamily_procedure;

opfamily_name | opfamily_procedure
-------------------- + --------------------
abstime_ops | hashint4
aclitem_ops | hash_aclitem
array_ops | hash_array
bool_ops | hashchar
...

Although not documented, these functions can be used to compute the hash code of a value of the appropriate type. For example, 
the hashtext function is used for the text_ops family:

demo = # select hashtext ('times');
hashtext
-----------
127722028
(1 row)

demo = # select hashtext ('two');
hashtext
-----------
345620034
(1 row)

Properties

Let's see the properties of the hash index that this accessor tells the system about itself. 
We gave requests last time; for now we will limit ourselves only to the results:

name            | pg_indexam_has_property
--------------- + -------------------------
can_order       | f
can_unique      | f
can_multi_col   | f
can_exclude     | t

name            | pg_index_has_property
--------------- + -----------------------
clusterable     | f
index_scan      | t
bitmap_scan     | t
backward_scan   | t

name                 | pg_index_column_has_property
-------------------- + ----------------------------- -
asc                  | f
desc                 | f
nulls_first          | f
nulls_last           | f
orderable            | f
distance_orderable   | f
returnable           | f
search_array         | f
search_nulls         | f

The hash function does not preserve the ordering relation: 
from the fact that the value of the hash function of one key is less than the value of the function of the other key, 
no conclusions can be drawn about how the keys themselves are ordered. Therefore, a hash index, in principle, can support a single "equal" operation:

demo = # select opf.opfname AS opfamily_name,
amop.amopopr :: regoperator AS opfamily_operator
from pg_am am,
pg_opfamily opf,
pg_amop amop
where opf.opfmethod = am.oid
and amop.amopfamily = opf.oid
and am.amname = 'hash'
order by opfamily_name,
opfamily_operator;

opfamily_name   | opfamily_operator
--------------- + ----------------------
abstime_ops     | = (abstime, abstime)
aclitem_ops     | = (aclitem, aclitem)
array_ops       | = (anyarray, anyarray)
bool_ops        | = (boolean, boolean)
...

Accordingly, the hash index cannot produce ordered data (can_order, orderable). For the same reason, 
the hash index does not work with null values: the "equal" operation does not make sense for null (search_nulls).
Since the hash index does not store the keys (but only the hash codes of the keys), it cannot be used for index-only access (returnable).
Multi-column indexes (can_multi_col) are not supported by this accessor.

Insides

Starting with version 10, it will be possible to look into the internal structure of the hash index using the pageinspect extension. This is how it will look:
https://www.postgresql.org/docs/devel/pageinspect.html

demo = # create extension pageinspect;
CREATE EXTENSION

Meta page (get the number of rows in the index and the maximum used cart number):

demo = # select hash_page_type (get_raw_page ('flights_flight_no_idx', 0));
hash_page_type
----------------
metapage
(1 row)
demo = # select ntuples, maxbucket from hash_metapage_info (get_raw_page ('flights_flight_no_idx', 0));
ntuples   | maxbucket
--------- + -----------
33121     | 127
(1 row)

Cart page (get the number of actual rows and rows that can be cleared):

demo = # select hash_page_type (get_raw_page ('flights_flight_no_idx', 1));
hash_page_type
----------------
bucket
(1 row)
demo = # select live_items, dead_items from hash_page_stats (get_raw_page ('flights_flight_no_idx', 1));
live_items   | dead_items
------------ + ------------
407          | 0
(1 row)

And so on. But you can hardly understand the meaning of all the available fields without studying the source code.

---------
- Btree -
---------

Device

The btree index, aka B-tree, is suitable for sorted data. In other words, the greater than, greater than or equal, less than, less than or equal, 
and equal operators must be defined for the data type. Note that the same data can sometimes be sorted in different ways, 
which brings us back to the concept of an operator family.

As always, the B-tree index records are packed into pages. In leaf pages, these records contain indexed data (keys) and table row references (TIDs); 
in internal pages, each record refers to a child index page and contains the minimum key value in that page.

B-trees have several important properties:

 * They are balanced, that is, any leaf page is separated from the root by the same number of internal pages. 
   Therefore, the search for any value takes the same time.
 * They are highly branched, that is, each page (usually 8 KB) contains many (hundreds) of TIDs at once. As a result, the depth of B-trees is shallow; 
   in practice, up to 4–5 for very large tables.
 * The data in the index is sorted in non-decreasing order (both between pages and within each page), 
   and pages at the same level are linked in a bidirectional list. Therefore, 
   we can get an ordered data set by simply traversing the list in one direction or the other, without going back to the root each time.

Here is a schematic example of an index on a single field with integer keys.
https://mega.nz/file/AwkGBYia#EzRFw9sdSBHF7iDZ44tw_t1hEr-_bLXB7GLeYWYN1sk

At the very beginning of the file is a meta page that refers to the root of the index. 
Below the root are internal nodes; the bottom row is leaf pages. Down arrows symbolize links from leaf nodes to table rows (TIDs).

Equality search

Consider finding a value in a tree using the "indexed-field = expression" condition. Let's say we are interested in key 49.
https://mega.nz/file/Y4k2GYZL#YG9rdbyE9V_TWzdfpKGV26RL_VsvgwS7UCBsGQECVRg

The search starts from the root node, and we need to determine which of the child nodes to go down to. Knowing the keys (4, 32, 64) in the root node, 
we thereby understand the ranges of values in the child nodes. Since 32 ≤ 49 <64, we need to descend to the second child node. 
Then the same procedure is repeated recursively until a leaf node is reached, from which the necessary TIDs can already be obtained.

In reality, this seemingly simple procedure is complicated by a number of circumstances. For example, an index may contain non-unique keys, 
and there may be enough identical values that they do not fit on one page. Continuing our example, 
it seems that from the internal node we should go down the link that leads from the value 49. But, as you can see in the picture, 
this way we will skip one of the keys 49 in the previous leaf page. Therefore, when we find an exact key equality on the inner page, 
we have to go down one position to the left, and then look through the index records of the lower level from left to right in search of the key of interest.

(Another difficulty is caused by the fact that during the search, other processes can change the data: the tree can be rebuilt, 
pages can be split into two, etc. All algorithms are built in such a way that these simultaneous actions do not interfere with each other and 
do not require unnecessary we will not go into these details.)

Inequality search

When searching for "indexed-field ≤ expression" (or "indexed-field ≥ expression"), 
first find the value in the index by the equality condition "indexed-field = expression" (if any), 
and then move through the leaf pages to end in the desired direction.
The figure illustrates this process for the condition n ≤ 35:
https://mega.nz/file/goMH0KYa#4LBkWOJoTHqPi7VEaFmkUr-4wBnU7si80TqX1l29_K0

The operators "greater than" and "less" are supported in the same way, you just need to exclude the originally found value.

Range search

When searching in the range "expression1 ≤ indexed-field ≤ expression2", we find the value by the condition "indexed-field = expression1", 
and then move through the leaf pages while the condition "indexed-field ≤ expression2" is satisfied. Or vice versa: 
we start with the second expression and move in the other direction until we reach the first.

The figure shows the process for condition 23 ≤ n ≤ 64:
https://mega.nz/file/4oFVnaCS#6myiocGQ5M4wT4P8FSO8r-Odm-4OnUxAXJembC-XNgY

Example

Let's see how query plans look like with an example. As usual, we will use the demo database and this time we will take the aircraft table. 
There are only nine rows in it, and the planner will not use the index of its own accord - after all, the entire table fits into one page. 
But we are interested in it because of its clarity.

demo = # select * from aircrafts;
aircraft_code   | model                 | range
--------------- + --------------------- + -------
773             | Boeing 777-300        | 11100
763             | Boeing 767-300        | 7900
SU9             | Sukhoi SuperJet-100   | 3000
320             | Airbus A320-200       | 5700
321             | Airbus A321-200       | 5600
319             | Airbus A319-100       | 6700
733             | Boeing 737-300        | 4200
CN1             | Cessna 208 Caravan    | 1200
CR2             | Bombardier CRJ-200    | 2700
(9 rows)

demo = # create index on aircrafts (range);
CREATE INDEX

demo = # set enable_seqscan = off;
SET

(Or explicitly create index on aircrafts using btree (range), but the B-tree is built by default.)

Equality search:

demo = # explain (costs off) select * from aircrafts where range = 3000;
QUERY PLAN
-------------------------------------------------- -
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: (range = 3000)
(2 rows)

Inequality search:

demo = # explain (costs off) select * from aircrafts where range <3000;
QUERY PLAN
-------------------------------------------------- -
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: (range <3000)
(2 rows)

And by range:

demo = # explain (costs off) select * from aircrafts where range between 3000 and 5000;
QUERY PLAN
-------------------------------------------------- ---
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: ((range> = 3000) AND (range <= 5000))
(2 rows)

Sorting

It is worth emphasizing again that for any scan method (index, index-only, bitmap), the btree accessor returns ordered data, 
as can be clearly seen in the above figures.
Therefore, if there is an index on a table by a sort condition, the optimizer will take into account both possibilities: 
accessing the table by index and automatically retrieving sorted data, or sequential reading of the table and then sorting the result.

The sort order

The sort order can be explicitly specified when creating an index. For example, a flight range index could be created like this:

demo = # create index on aircrafts (range desc);

In this case, large values would appear on the left in the tree, and smaller values on the right. 
Why would you need it if you can walk through the indexed values both in one direction and in the other?

The reason is multi-column indices. Let's create a view that will show the aircraft models and the conventional division into short, 
medium and long haul vessels:

demo = # create view aircrafts_v as
select model,
case
when range <4000 then 1
when range <10000 then 2
else 3
end as class
from aircrafts;
CREATE VIEW

demo = # select * from aircrafts_v;
model                 | class
--------------------- + -------
Boeing 777-300        | 3
Boeing 767-300        | 2
Sukhoi SuperJet-100   | 1
Airbus A320-200       | 2
Airbus A321-200       | 2
Airbus A319-100       | 2
Boeing 737-300        | 2
Cessna 208 Caravan    | 1
Bombardier CRJ-200    | 1
(9 rows)

And create an index (using an expression):

demo = # create index on aircrafts (
(case when range <4000 then 1 when range <10000 then 2 else 3 end), model);
CREATE INDEX

We can now use this index to get the data sorted by both columns in ascending order:

demo = # select class, model from aircrafts_v order by class, model;
class   | model
------- + ---------------------
1       | Bombardier CRJ-200
1       | Cessna 208 caravan
1       | Sukhoi SuperJet-100
2       | Airbus A319-100
2       | Airbus A320-200
2       | Airbus A321-200
2       | Boeing 737-300
2       | Boeing 767-300
3       | Boeing 777-300
(9 rows)

demo = # explain (costs off) select class, model from aircrafts_v order by class, model;
QUERY PLAN
-------------------------------------------------- ------
Index Scan using aircrafts_case_model_idx on aircrafts
(1 row)

Similarly, you can execute a query sorted in descending order:

demo = # select class, model from aircrafts_v order by class desc, model desc;
class   | model
------- + ---------------------
3       | Boeing 777-300
2       | Boeing 767-300
2       | Boeing 737-300
2       | Airbus A321-200
2       | Airbus A320-200
2       | Airbus A319-100
1       | Sukhoi SuperJet-100
1       | Cessna 208 caravan
1       | Bombardier CRJ-200
(9 rows)

demo = # explain (costs off)
select class, model from aircrafts_v order by class desc, model desc;
QUERY PLAN
-------------------------------------------------- ---------------
Index Scan Backward using aircrafts_case_model_idx on aircrafts
(1 row)

But it is not possible to retrieve data from this index sorted in descending order by one column and ascending by the other. 
This requires a separate sorting:

demo = # explain (costs off)
select class, model from aircrafts_v order by class asc, model desc;
QUERY PLAN
-------------------------------------------------
Sort
Sort Key: (CASE ... END), aircrafts.model DESC
-> Seq Scan on aircrafts
(3 rows)

(Note that out of grief, the planner chose to scan the table, even though the enable_seqscan = off setting was made earlier. 
This is because it does not actually disable table scans, but only sets it at a prohibitive cost - see the "costs on" plan. )

For such a query to be executed using an index, the index must be built with sorting in the desired order:

demo = # create index aircrafts_case_asc_model_desc_idx on aircrafts (
(case when range <4000 then 1 when range <10000 then 2 else 3 end) asc, model desc);
CREATE INDEX

demo = # explain (costs off)
select class, model from aircrafts_v order by class asc, model desc;
QUERY PLAN
-------------------------------------------------- ---------------
Index Scan using aircrafts_case_asc_model_desc_idx on aircrafts
(1 row)

Column order

Another issue that arises when using multi-column indexes is the order in which the columns are listed in the index. 
In the case of a B-tree, this order is of great importance: the data within the pages will be sorted first by the first field, then by the second, and so on.
The index that we have built on the basis of range intervals and models can be roughly represented as follows:
https://mega.nz/file/18kjDACJ#-XW86IeTpistZ6NpktznoL6_XZOb4BBBAnMHeuk51Ko

Of course, such a small index will actually fit into one root page; in the figure, it is artificially distributed over several pages for clarity.

It is clear from this diagram that search will work efficiently with, for example, 
predicates such as "class = 3" (search only in the first field) or "class = 3 and model = 'Boeing 777-300'" (search in both fields ).

But the search by the predicate "model = 'Boeing 777-300'" will be much less efficient: starting from the root, 
we cannot determine which of the child nodes to go down to, so we will have to go down into all of them. 
This does not mean that such an index cannot be used in principle - the only question is efficiency. 
For example, if we had three classes of aircraft and very many models in each class, then we would have to scan about a third of the index, 
and this might be more efficient than a full scan of the table. It might not have happened.

But if you create an index like this:

demo = # create index on aircrafts (
model, (case when range <4000 then 1 when range <10000 then 2 else 3 end));
CREATE INDEX

then the order of the fields will change:
https://mega.nz/file/t8tj0SiC#Q35bEXEb5bDPS-dhzpa_wHj2S8IwEfugAaELi4SGppw
And with such an index, the search by the predicate "model = 'Boeing 777-300'" will be performed efficiently, but by the predicate "class = 3" it will not.

Undefined values

The btree accessor indexes undefined values ​​and supports is null and is not null searches.

Let's take a table of flights with undefined values:

demo = # create index on flights (actual_arrival);
CREATE INDEX
demo = # explain (costs off) select * from flights where actual_arrival is null;
QUERY PLAN
-------------------------------------------------- -----
Bitmap Heap Scan on flights
Recheck Cond: (actual_arrival IS NULL)
-> Bitmap Index Scan on flights_actual_arrival_idx
Index Cond: (actual_arrival IS NULL)
(4 rows)

Undefined values are positioned at one end or the other of leaf nodes, depending on how the index was created (nulls first or nulls last). 
This is important if the query involves sorting: the order of undefined values ​​in the index and in the sort order must match for the index to be usable.
In this example, the orders are the same, so the index can be used:

demo = # explain (costs off) select * from flights order by actual_arrival nulls last;
QUERY PLAN
-------------------------------------------------- ------
Index Scan using flights_actual_arrival_idx on flights
(1 row)

But here the orders are different, and the optimizer chooses to scan the table and sort:

demo = # explain (costs off) select * from flights order by actual_arrival nulls first;
QUERY PLAN
----------------------------------------
Sort
Sort Key: actual_arrival NULLS FIRST
-> Seq Scan on flights
(3 rows)

For the index to be used, you need to create it so that undefined values come at the beginning:

demo = # create index flights_nulls_first_idx on flights (actual_arrival nulls first);
CREATE INDEX
demo = # explain (costs off) select * from flights order by actual_arrival nulls first;
QUERY PLAN
-------------------------------------------------- ---
Index Scan using flights_nulls_first_idx on flights
(1 row)

The reason for such discrepancies, of course, is that undefined values are not sortable: 
the result of comparing an undefined value with any other is undefined:

demo = # \ pset null NULL
Null display is "NULL".
demo = # select null <42;
? column?
----------
NULL
(1 row)

This goes against the essence of the B-tree and does not fit into the general scheme. 
But undefined values play such an important role in databases that exceptions must be made for them all the time.

The consequence of the fact that undefined values are indexed is the ability to use the index even if no conditions are imposed on the table at all 
(since the index is guaranteed to contain information about all the rows of the table). 
This can make sense if you want to order the data in your query and the index provides the desired order. 
Then the planner may prefer index access to save on separate sorting.

Properties

Let's see the properties of the btree accessor (requests were given earlier).

amname | name | pg_indexam_has_property
-------- + --------------- + -------------------------
btree | can_order | t
btree | can_unique | t
btree | can_multi_col | t
btree | can_exclude | t

As we have seen, the B-tree can order data and maintains uniqueness — and it is the only accessor that provides such properties. 
Multiple column indexes are also valid; but other methods can do it too (though not all). 
We will talk about support for exclude constraints next time, for good reason.

name | pg_index_has_property
--------------- + -----------------------
clusterable | t
index_scan | t
bitmap_scan | t
backward_scan | t

The b-tree accessor supports both an index scan and a bitmap scan. And, as we have seen, 
it is able to walk the tree both "forward" and in the opposite direction.

name | pg_index_column_has_property
-------------------- + ----------------------------- -
asc | t
desc | f
nulls_first | f
nulls_last | t
orderable | t
distance_orderable | f
returnable | t
search_array | t
search_nulls | t

The first four properties of this level tell you exactly how the values of this particular column are ordered. In this example, 
values are sorted in ascending order (asc) and nulls are sorted to the end (nulls_last). But, as we have seen, there may be other combinations.

The search_array property indicates that the index supports the following constructs:

demo = # explain (costs off)
select * from aircrafts where aircraft_code in ('733', '763', '773');
QUERY PLAN
-------------------------------------------------- ---------------
Index Scan using aircrafts_pkey on aircrafts
Index Cond: (aircraft_code = ANY ('{733,763,773}' :: bpchar []))
(2 rows)

The returnable property speaks of support for only index scanning - which is logical, 
because indexed values themselves are stored in index records (as opposed to a hash index, for example). 
It is appropriate to say a few words here about the peculiarities of B-tree covering indexes.

Unique indexes with additional columns

Kak my govorili raneye, pokryvayushchim nazyvayetsya indeks, kotoryy soderzhit vse znacheniya, neobkhodimyye v zaprose; 
pri etom obrashcheniye k samoy tablitse uzhe ne trebuyetsya (pochti). V tom chisle pokryvayushchim mozhet byt' i unikal'nyy indeks.

No dopustim, my khotim dobavit' k unikal'nomu indeksu dopolnitel'nyye stolbtsy, neobkhodimyye v zaprose. 
No unikal'nost' takikh sostavnykh znacheniy ne garantiruyet unikal'nost' klyucha, poetomu nam potrebuyetsya imet' dva indeksa prakticheski po odnim i tem zhe stolbtsam: odin unikal'nyy dlya podderzhki ogranicheniya tselostnosti i yeshche odin v kachestve pokryvayushchego. Eto, konechno, neeffektivno.

V nashey kompanii Anastasiya Lubennikova lubennikovaav dorabotala metod btree tak, 
chtoby v unikal'nyy indeks mozhno bylo vklyuchat' dopolnitel'nyye — neunikal'nyye — stolbtsy. Nadeyemsya, 
chto etot patch budet prinyat soobshchestvom i voydet v PostgreSQL, no eto sluchitsya uzhe ne v versii 10. 
Poka patch dostupen v Postgres Pro Standard 9.5+, i vot kak eto vyglyadit.

Voz'mem tablitsu bronirovaniy:

demo=# \d bookings
Table "bookings.bookings"
Column        | Type                     | Modifiers
--------------+--------------------------+-----------
book_ref      | character(6)             | not null
book_date     | timestamp with time zone | not null
total_amount  | numeric(10,2)            | not null
Indexes:
"bookings_pkey" PRIMARY KEY, btree (book_ref)
Referenced by:
TABLE "tickets" CONSTRAINT "tickets_book_ref_fkey" FOREIGN KEY (book_ref) REFERENCES bookings(book_ref)

V ney pervichnyy klyuch (book_ref, kod bronirovaniya) podderzhan obychnym btree-indeksom. Sozdadim novyy unikal'nyy indeks s dopolnitel'nym stolbtsom:

demo=# create unique index bookings_pkey2 on bookings(book_ref) include (book_date);
CREATE INDEX

Teper' zamenim sushchestvuyushchiy indeks na novyy (v tranzaktsii, chtoby vse izmeneniya vstupili v silu odnovremenno):

demo=# begin;
BEGIN
demo=# alter table bookings drop constraint bookings_pkey cascade;
NOTICE: drop cascades to constraint tickets_book_ref_fkey on table tickets
ALTER TABLE
demo=# alter table bookings add primary key using index bookings_pkey2;
ALTER TABLE
demo=# alter table tickets add foreign key (book_ref) references bookings (book_ref);
ALTER TABLE
demo=# commit;
COMMIT

Vot chto poluchilos':

demo=# \d bookings
Table "bookings.bookings"
Column        | Type                     | Modifiers
--------------+--------------------------+-----------
book_ref      | character(6)             | not null
book_date     | timestamp with time zone | not null
total_amount  | numeric(10,2)            | not null
Indexes:
"bookings_pkey2" PRIMARY KEY, btree (book_ref) INCLUDE (book_date)
Referenced by:
TABLE "tickets" CONSTRAINT "tickets_book_ref_fkey" FOREIGN KEY (book_ref) REFERENCES bookings(book_ref)

Teper' odin i tot zhe indeks rabotayet i kak unikal'nyy, i vystupayet pokryvayushchim dlya takogo, naprimer, zaprosa:

demo=# explain(costs off)
select book_ref, book_date from bookings where book_ref = '059FC4';
QUERY PLAN
--------------------------------------------------
Index Only Scan using bookings_pkey2 on bookings
Index Cond: (book_ref = '059FC4'::bpchar)
(2 rows)

As we said earlier, a covering index is an index that contains all the values needed in a query; in this case, 
access to the table itself is no longer required (almost). A unique index can also be covered.

But let's say we want to add additional columns to the unique index that are needed in the query. 
But the uniqueness of such composite values does not guarantee the uniqueness of the key, so we need to have two indexes on practically the same columns: 
one unique to support the integrity constraint and one as a covering. This is, of course, ineffective.

In our company, Anastasia Lubennikova lubennikovaav has improved the btree method so that additional - non-unique - columns can be included in a unique index. 
Hopefully this patch will be accepted by the community and go into PostgreSQL, but it won't be in version 10. 
The patch is currently available in Postgres Pro Standard 9.5+, and this is how it looks.

Let's take a table of reservations:

demo = # \ d bookings
Table "bookings.bookings"
Column         | Type                       | Modifiers
-------------- + -------------------------- + -----------
book_ref       | character (6)              | not null
book_date      | timestamp with time zone   | not null
total_amount   | numeric (10,2)             | not null
Indexes:
"bookings_pkey" PRIMARY KEY, btree (book_ref)
Referenced by:
TABLE "tickets" CONSTRAINT "tickets_book_ref_fkey" FOREIGN KEY (book_ref) REFERENCES bookings (book_ref)

In it, the primary key (book_ref, reservation code) is backed by a regular btree index. Let's create a new unique index with an additional column:

demo = # create unique index bookings_pkey2 on bookings (book_ref) include (book_date);
CREATE INDEX

Now let's replace the existing index with a new one (in a transaction so that all changes take effect at the same time):

demo = # begin;
BEGIN
demo = # alter table bookings drop constraint bookings_pkey cascade;
NOTICE: drop cascades to constraint tickets_book_ref_fkey on table tickets
ALTER TABLE
demo = # alter table bookings add primary key using index bookings_pkey2;
ALTER TABLE
demo = # alter table tickets add foreign key (book_ref) references bookings (book_ref);
ALTER TABLE
demo = # commit;
COMMIT

Here's what happened:

demo = # \ d bookings
Table "bookings.bookings"
Column         | Type                       | Modifiers
-------------- + -------------------------- + -----------
book_ref       | character (6)              | not null
book_date      | timestamp with time zone   | not null
total_amount   | numeric (10,2)             | not null
Indexes:
"bookings_pkey2" PRIMARY KEY, btree (book_ref) INCLUDE (book_date)
Referenced by:
TABLE "tickets" CONSTRAINT "tickets_book_ref_fkey" FOREIGN KEY (book_ref) REFERENCES bookings (book_ref)

Now the same index works as a unique one and acts as a covering for such, for example, a query:

demo = # explain (costs off)
select book_ref, book_date from bookings where book_ref = '059FC4';
QUERY PLAN
--------------------------------------------------
Index Only Scan using bookings_pkey2 on bookings
Index Cond: (book_ref = '059FC4' :: bpchar)
(2 rows)

Index creation

A well-known, but no less important fact: it is better to load a large amount of data into a table without indexes, 
and create the necessary indexes afterwards. Not only is this faster, but the index itself is likely to be smaller.

The fact is that when creating a btree index, a more efficient procedure is used than inserting values row by row into the tree. 
Roughly speaking, all data in the table is sorted and the leaf index pages are formed from them; 
then the inner pages are "built up" on this base until the whole pyramid converges to the root.

The speed of this process depends on the size of the available RAM, which is limited by the maintenance_work_mem parameter, 
so increasing the value may result in a speedup. In the case of unique indexes, in addition to maintenance_work_mem, work_mem memory is also allocated.

Comparison semantics

Last time we talked about how PostgreSQL needs to know which hash functions to call for values of different types, 
and what this mapping is stored in the hash accessor. Likewise, the system needs to understand how to order values - this is necessary for sorts, 
groupings (sometimes), merge joins, etc. PostgreSQL does not bind to operator names (such as>, <, =), 
because the user can define your data type and name the corresponding operators something else. 
Instead, operator names are defined by a family of operators bound to the btree accessor.

For example, here are the comparison operators used in the bool_ops family:

postgres = # select amop.amopopr :: regoperator as opfamily_operator,
amop.amopstrategy
from pg_am am,
pg_opfamily opf,
pg_amop amop
where opf.opfmethod = am.oid
and amop.amopfamily = opf.oid
and am.amname = 'btree'
and opf.opfname = 'bool_ops'
order by amopstrategy;

opfamily_operator      | amopstrategy
---------------------- + --------------
<(boolean, boolean)    | 1
<= (boolean, boolean)  | 2
= (boolean, boolean)   | 3
> = (boolean, boolean) | 4
> (boolean, boolean)   | five
(5 rows)

Here we see five comparison operators, but, as said, should not be guided by their names. To understand which comparison is implemented by which operator, 
the concept of strategy is introduced. For btree, five strategies are defined that define the semantics of the operators:
1 - less;
2 - less or equal;
3 - equal;
4 - more or equal;
5 - more.

Some families can contain several operators that implement the same strategy. For example, here are the operators in the integer_ops family for strategy 1:

postgres = # select amop.amopopr :: regoperator as opfamily_operator
from pg_am am,
pg_opfamily opf,
pg_amop amop
where opf.opfmethod = am.oid
and amop.amopfamily = opf.oid
and am.amname = 'btree'
and opf.opfname = 'integer_ops'
and amop.amopstrategy = 1
order by opfamily_operator;

opfamily_operator
----------------------
<(integer, bigint)
<(smallint, smallint)
<(integer, integer)
<(bigint, bigint)
<(bigint, integer)
<(smallint, integer)
<(integer, smallint)
<(smallint, bigint)
<(bigint, smallint)
(9 rows)

Due to this, the optimizer is able to compare values of different types belonging to the same family without resorting to casting.

Index support for new data type

Index support for new data type The documentation has https://www.postgresql.org/docs/13/xindex.html 
creating a new data type for complex numbers, and an operator class for sorting values of that type. 
This example uses the C language, which is absolutely justified when speed is important. 
But nothing prevents you from doing the same experiment in pure SQL, just to try and better understand the semantics of comparison.

Create a new composite type with two fields: real and imaginary parts.

postgres = # create type complex as (re float, im float);
CREATE TYPE

You can create a table with a field of a new type and add some values ​​to it:

postgres = # create table numbers (x complex);
CREATE TABLE
postgres = # insert into numbers values ​​((0.0, 10.0)), ((1.0, 3.0)), ((1.0, 1.0));
INSERT 0 3

Now the question arises: how to order complex numbers if, in the mathematical sense, an order relation is not defined for them?

As it turns out, the comparison operations are already defined for us:

postgres = # select * from numbers order by x;
x
--------
(0.10)
(1.1)
(1,3)
(3 rows)

By default, a composite type is sorted componentwise: first the first fields are compared, then the second, and so on, 
much like text strings are compared character by character. But another order can be defined. For example, 
complex numbers can be viewed as vectors and ordered modulo (length), 
which is calculated as the root of the sum of the squares of the coordinates (Pythagoras' theorem). 
To define this order, let's create a helper function to calculate the modulus:

postgres = # create function modulus (a complex) returns float as $$
select sqrt (a.re * a.re + a.im * a.im);
$$ immutable language sql;
CREATE FUNCTION

And with its help, we will methodically define functions for all five comparison operations:

postgres = # create function complex_lt (a complex, b complex) returns boolean as $$
select modulus (a) <modulus (b);
$$ immutable language sql;
CREATE FUNCTION
postgres = # create function complex_le (a complex, b complex) returns boolean as $$
select modulus (a) <= modulus (b);
$$ immutable language sql;
CREATE FUNCTION
postgres = # create function complex_eq (a complex, b complex) returns boolean as $$
select modulus (a) = modulus (b);
$$ immutable language sql;
CREATE FUNCTION
postgres = # create function complex_ge (a complex, b complex) returns boolean as $$
select modulus (a)> = modulus (b);
$$ immutable language sql;
CREATE FUNCTION
postgres = # create function complex_gt (a complex, b complex) returns boolean as $$
select modulus (a)> modulus (b);
$$ immutable language sql;
CREATE FUNCTION

And let's create the appropriate operators. To show that they don't have to be called ">", "<" and so on, let's give them "weird" names.

postgres = # create operator # <# (leftarg = complex, rightarg = complex, procedure = complex_lt);
CREATE OPERATOR
postgres = # create operator # <= # (leftarg = complex, rightarg = complex, procedure = complex_le);
CREATE OPERATOR
postgres = # create operator # = # (leftarg = complex, rightarg = complex, procedure = complex_eq);
CREATE OPERATOR
postgres = # create operator #> = # (leftarg = complex, rightarg = complex, procedure = complex_ge);
CREATE OPERATOR
postgres = # create operator #> # (leftarg = complex, rightarg = complex, procedure = complex_gt);
CREATE OPERATOR

At this stage, you can already compare numbers:

postgres = # select (1.0,1.0) :: complex # <# (1.0,3.0) :: complex;
? column?
----------
t
(1 row)

In addition to the five operators, the btree accessor requires another (redundant, but convenient) 
function to be defined: it must return -1, 0, or 1 if the first value is less than, equal to, or greater than the second. 
Such an auxiliary function is called support; other accessors may require the creation of different support functions.

postgres = # create function complex_cmp (a complex, b complex) returns integer as $$
select case when modulus (a) <modulus (b) then -1
when modulus (a)> modulus (b) then 1
else 0
end;
$$ language sql;
CREATE FUNCTION

Now we are ready to create an operator class (and the family of the same name will be created automatically):

postgresx = # create operator class complex_ops
default for type complex
using btree as
operator 1 # <#,
operator 2 # <= #,
operator 3 # = #,
operator 4 #> = #,
operator 5 #> #,
function 1 complex_cmp (complex, complex);
CREATE OPERATOR CLASS

The sort now works as we wanted:

postgres = # select * from numbers order by x;
x
--------
(1.1)
(1,3)
(0.10)
(3 rows)

And, of course, it will be supported by the btree index.

For completeness, the support functions can be seen with the following query:

postgres = # select amp.amprocnum,
amp.amproc,
amp.amproclefttype :: regtype,
amp.amprocrighttype :: regtype
from pg_opfamily opf,
pg_am am,
pg_amproc amp
where opf.opfname = 'complex_ops'
and opf.opfmethod = am.oid
and am.amname = 'btree'
and amp.amprocfamily = opf.oid;

amprocnum       | amproc        | amproclefttype   | amprocrighttype
--------------- + ------------- + ---------------- + -----------------
1               | complex_cmp   | complex          | complex
(1 row)

Insides

The internal structure of the B-tree can be examined using the pageinspect extension.

demo = # create extension pageinspect;
CREATE EXTENSION

Index meta page:

demo = # select * from bt_metap ('ticket_flights_pkey');
magic    | version   | root   | level   | fastroot   | fastlevel
-------- + --------- + ------ + ------- + ---------- + -----------
340322   | 2         | 164    | 2       | 164        | 2
(1 row)

The most interesting thing here is the depth of the index (level): 
it took only 2 levels (excluding the root) to place an index in two columns for a table with a million rows.

Statistical information about block 164 (root):

demo = # select type, live_items, dead_items, avg_item_size, page_size, free_size
from bt_page_stats ('ticket_flights_pkey', 164);
type   | live_items   | dead_items   | avg_item_size   | page_size    | free_size
------ + ------------ + ------------ + --------------- + - ---------- + -----------
r      | 33           | 0            | 31              | 8192         | 6984
(1 row)

And the data itself in the block (in the data field, which is sacrificed here for the screen width, is the value of the indexing key in binary form):

demo = # select itemoffset, ctid, itemlen, left (data, 56) as data
from bt_page_items ('ticket_flights_pkey', 164) limit 5;
itemoffset   | ctid      | itemlen   | data
------------ + --------- + --------- + ---------------------------------------
1            | (3.1)     | 8         |
2            | (163.1)   | 32        | 1d 30 30 30 35 34 33 32 33 30 35 37 37 31 00 00 ff 5f 00
3            | (323.1)   | 32        | 1d 30 30 30 35 34 33 32 34 32 33 36 36 32 00 00 4f 78 00
4            | (482.1)   | 32        | 1d 30 30 30 35 34 33 32 35 33 30 38 39 33 00 00 4d 1e 00
5            | (641,1)   | 32        | 1d 30 30 30 35 34 33 32 36 35 35 37 38 35 00 00 2b 09 00
(5 rows)

The first element is technical in nature and sets the upper bound for the key value of all block elements (an implementation detail that we did not talk about),
and the data itself starts from the second element. You can see that the leftmost child is block 163, then block 323, and so on. Which, 
in turn, can also be studied using these same functions.

--------
- GiST -
--------

GiST is short for generalized search tree. This is a balanced search tree, just like the b-tree discussed earlier.

What's the difference? The b-tree index is tightly tied to the semantics of comparison: support for the operators "greater than", "less", "equal" 
is all it can do (but it can do very well!). But modern databases also store such data types for which these operators simply do not make sense: geodata, 
text documents, pictures ...
This is where the GiST index method comes in. It allows you to specify the principle of distribution of data of an arbitrary type over a balanced tree, 
and the method of using this representation for access by some operator. For example, in a GiST index, 
you can "stack" an R-tree for spatial data with support for positional operators (left, right; contains, etc.), 
or an RD-tree for sets with support for intersection or occurrence operators.

Extensibility in PostgreSQL makes it possible to create a completely new access method from scratch: to do this, 
you need to implement an interface with an indexing mechanism. But this requires thinking over not only the indexing logic, but also the page structure, 
effective implementation of locks, support of the write-ahead log - which implies a very high developer qualifications and great laboriousness. 
GiST simplifies the task by taking on low-level problems and providing its own interface: several functions that are not technical, but application domain. 
In this sense, we can say that GiST is a framework for building new access methods.

Device

GiST is a height-balanced tree of page nodes. Nodes are composed of index records.
Each record of a leaf node (leaf record) contains, in its most general form, a predicate (logical expression) and a reference to a table row (TID). 
Indexed data (key) must satisfy this predicate.
Each internal node record (internal record) also contains a predicate and a reference to a child node, 
and all indexed data of the child subtree must satisfy this predicate. In other words, an internal record predicate includes predicates for all child records. 
This is an important property that replaces the simple B-tree ordering of the GiST index.

GiST tree search uses a special function consistent, one of the interface-defined functions that is implemented differently for each supported operator family.
The consistency function is called on an index record and determines whether the predicate of the given record "matches" the search condition 
(of the form "indexed-field operator expression"). For internal records, it actually determines whether to descend into the appropriate subtree, 
and for leaf records, whether the indexed data satisfies the condition.

The search, as usual in the tree, starts from the root node. The consistency function is used to find out which child nodes it makes sense to visit 
(there may be several of them), and which - not. Then the algorithm is repeated for each of the found child nodes. If the node is leaf, 
then the record selected by the consistency function is returned as one of the results.

The search is done in depth: the algorithm first of all tries to get to some leaf node. This allows you to return the first results as quickly as possible 
(which can be important if the user is not interested in all the results, but only a few).

Note again that the consistency function should not have anything to do with the greater than, less than, or equal to operators. 
Its semantics may be completely different, and therefore the index is not expected to return values in any particular order.

We will not consider the algorithms for inserting and deleting values in GiST - several more interface functions are used for this. 
But there is one important point. When a new value is inserted into the index, 
a parent record is selected for it so that its predicate would have to be expanded as little as possible (ideally, not expanded at all). 
But when the value is removed, the predicate of the parent record is no longer narrowed. This only happens in two cases: when the page is split in two 
(when there is not enough space on the page to insert a new index record) and when the index is completely rebuilt (with reindex or vacuum full). 
Therefore, the performance of the GiST index with frequently changing data can degrade over time.

Next, we will look at a few examples of indexes for different data types and useful properties of GiST:

 * points (and other geometric objects) and search for nearest neighbors;
 * intervals and exclusion limits;
 * full-text search.

R-tree for points

Let us demonstrate the above using the example of an index for points on a plane (similar indexes can be constructed for other geometric objects). 
A regular B-tree is not suitable for this data type because no comparison operators are defined for points.
The idea of an R-tree is that the plane is divided into rectangles, which together cover all indexed points. 
The index record stores the rectangle, and the predicate can be formulated as: "the required point lies within the given rectangle."
The root of the R-tree will contain some of the largest rectangles (possibly even intersecting). 
Child nodes will contain smaller rectangles nested within the parent, collectively enclosing all underlying points.
Leaf nodes, in theory, should contain indexed points, but the data type in all index records must be the same; therefore, 
all the same rectangles are kept, but "collapsed" to points.
To visualize such a structure, below are pictures of three levels of the R-tree; the points represent the coordinates of airports 
(similar to the airports table in the demo database, but more data from openflights.org is taken here).

https://mega.nz/file/4ttxECwS#jFtAUMQs6xZqAlg4W_aNXkxqi0pQheP4Z4w2MVVgHAY
First level; two large intersecting rectangles are visible.

https://mega.nz/file/MxlhmQRB#iM0N3iJbpxuLdlesK_N63WNIaGesJZyYd7xiMU3qqVg
Second level; large rectangles break up into smaller areas.

https://mega.nz/file/BotzhYBC#Tk1vFhMPN_BxwmOqKcTmWJrNyQQOaal8n4Pr4tmVY_k
Third level; each rectangle contains as many points as they fit on one index page.

Let us now consider in more detail a very simple "single-level" example:
https://mega.nz/file/Ew0zSSAb#VnH4ZA6CzoqsoX-W-ZIHaryPbYwiTh18sDPZVQxl-F0

postgres=# create table points(p point);
CREATE TABLE
postgres=# insert into points(p) values
  (point '(1,1)'), (point '(3,2)'), (point '(6,3)'),
  (point '(5,5)'), (point '(7,8)'), (point '(8,6)');
INSERT 0 6
postgres=# create index on points using gist(p);
CREATE INDEX

The structure of the index with such a partition will look like this:
https://mega.nz/file/lk0zlSDQ#-Ylkeo937-vO6KJ_62IdGk4hnj4GvGXTbAdfLT58Hjw

The created index can be used to speed up, for example, such a query: "find all points included in the given rectangle." 
This condition is formulated as follows: p <@ box '(2,1), (6,3)' (the operator <@ from the points_ops family means "is contained in"):

postgres=# set enable_seqscan = off;
SET
postgres=# explain(costs off) select * from points where p <@ box '(2,1),(7,4)';
                  QUERY PLAN                  
----------------------------------------------
 Index Only Scan using points_p_idx on points
   Index Cond: (p <@ '(7,4),(2,1)'::box)
(2 rows)

The consistency function for such an operator ("indexed-field <@ expression" where indexed-field is a point and expression is a rectangle) is defined as follows.
For internal notation, it returns yes if its rectangle intersects with the rectangle specified by the expression. 
For sheet notation, the function returns yes if its point (the collapsed rectangle) is contained in the rectangle specified by the expression.
https://mega.nz/file/w8tFDYCK#X91VkhzKHGYQ_KbfDSYX0-vwb8hO9FBv2QsJ_AW9IiE

The search starts from the root node. Rectangle (2,1) - (7,4) intersects with (1,1) - (6,3), but does not intersect with (5,5) - (8,8), 
so there is no need to descend into the second subtree.
https://mega.nz/file/498zxYaA#DN_i3iuUyHcWxvvGnv0y7rdpRCQzPHymoKtDnSwD80U

Arriving at the leaf node, we iterate over the three points contained there and return two of them as the result: (3,2) and (6,3).
postgres=# select * from points where p <@ box '(2,1),(7,4)';
   p  
-------
 (3,2)
 (6,3)
(2 rows)

Inside

Regular pageinspect, alas, does not allow you to look inside the GiST index. 
But there is another way - the gevel extension. It is not included in the standard delivery; see installation instructions.

If done correctly, you will have three functions available. First, some statistics:

postgres=# select * from gist_stat('airports_coordinates_idx');
                gist_stat                
------------------------------------------
 Number of levels:          4            +
 Number of pages:           690          +
 Number of leaf pages:      625          +
 Number of tuples:          7873         +
 Number of invalid tuples:  0            +
 Number of leaf tuples:     7184         +
 Total size of tuples:      354692 bytes +
 Total size of leaf tuples: 323596 bytes +
 Total size of index:       5652480 bytes+
 
(1 row)

Here you can see that the index for the coordinates of the airport took 690 pages and consists of four levels: 
the root and two inner levels were shown above in the figures, and the fourth level is leaf.
In fact, the index for eight thousand points will take up much less space: here, for clarity, it was created with a filling of 10%.
Second, you can display the index tree:

postgres=# select * from gist_tree('airports_coordinates_idx');
                                       gist_tree                                              
-----------------------------------------------------------------------------------------
 0(l:0) blk: 0 numTuple: 5 free: 7928b(2.84%) rightlink:4294967295 (InvalidBlockNumber) +
     1(l:1) blk: 335 numTuple: 15 free: 7488b(8.24%) rightlink:220 (OK)                 +
         1(l:2) blk: 128 numTuple: 9 free: 7752b(5.00%) rightlink:49 (OK)               +
             1(l:3) blk: 57 numTuple: 12 free: 7620b(6.62%) rightlink:35 (OK)           +
             2(l:3) blk: 62 numTuple: 9 free: 7752b(5.00%) rightlink:57 (OK)            +
             3(l:3) blk: 72 numTuple: 7 free: 7840b(3.92%) rightlink:23 (OK)            +
             4(l:3) blk: 115 numTuple: 17 free: 7400b(9.31%) rightlink:33 (OK)          +
 ...
 
 And thirdly, you can display the data itself, which is stored in index records. 
 Subtle point: the result of the function must be converted to the desired data type. 
 In our case, this type is box (bounding rectangle). For example, at the top level, we see five entries:
 
 postgres=# select level, a from gist_print('airports_coordinates_idx')
  as t(level int, valid bool, a box) where level = 1;
 level |                                   a                                  
-------+-----------------------------------------------------------------------
     1 | (47.663586,80.803207),(-39.2938003540039,-90)
     1 | (179.951004028,15.6700000762939),(15.2428998947144,-77.9634017944336)
     1 | (177.740997314453,73.5178070068359),(15.0664,10.57970047)
     1 | (-77.3191986083984,79.9946975708),(-179.876998901,-43.810001373291)
     1 | (-39.864200592041,82.5177993774),(-81.254096984863,-64.2382965088)
(5 rows)

Actually, the figures above were prepared on the basis of this data.

Search and ordering operators

The operators discussed so far (such as <@ in the predicate p <@ box '(2,1), (7,4)') can be called search operators because they specify the search 
conditions in the query.

There is another type of operator - ordering. They are used to specify the order of the returned results in the order by clause, 
where simple field specification is usually used. Here's an example of such a request:

postgres = # select * from points order by p <-> point '(4,7)' limit 2;
   p
-------
 (5.5)
 (7.8)
(2 rows)

Here p <-> point '(4,7)' is an expression using the ordering operator <->, which denotes the distance from one argument to another. 
The meaning of the request: to return two points closest to the point (4,7). This search is known as k-NN - k-nearest neighbor search.

To support this kind of queries, the accessor must define an additional distance function, 
and the ordering operator must be included in the appropriate operator class (for example, for points, the points_ops class). 
Here is a query that shows operators and their type (s - search, o - ordering):

postgres = # select amop.amopopr :: regoperator, amop.amoppurpose, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'point_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'gist'
and amop.amoplefttype = opc.opcintype;
      amopopr        | amoppurpose   | amopstrategy
-------------------- + ------------- + --------------
 << (point, point)   | s             | 1 strictly left
 >> (point, point)   | s             | 5 strictly right
 ~ = (point, point)  | s             | 6 matches
 <^ (point, point)   | s             | 10 strictly from below
 > ^ (point, point)  | s             | 11 strictly on top
 <-> (point, point)  | o             | 15 distance
 <@ (point, box)     | s             | 28 is contained in a rectangle
 <@ (point, polygon) | s             | 48 contained in landfill
 <@ (point, circle)  | s             | 68 contained in a circle
(9 rows)

It also shows the numbers of strategies with an explanation of their meaning. It can be seen that there are much more strategies than btree; 
only some of them are supported for points. For other types of data, other strategies may be defined.

The distance function is called on an index element and must determine the distance (based on operator semantics) 
from the value specified by the expression ("indexed-field ordering-operator expression") to the given element. For a leaf item, 
it is simply the distance to the indexed value. For an inner element, the function should return the minimum of the distances to leaf children. 
Since it would be very expensive to iterate over all the child records, the function has the right to optimistically underestimate the distance, 
but at the cost of worsening the search efficiency. However, it should not be categorically exaggerated - this will disrupt the work of the index.

The distance function can return any sortable value (for ordering, PostgreSQL will use comparison semantics from the appropriate btree accessor operator family,
as described earlier).

In the case of points on a plane, the distance is understood in the most ordinary sense: the value of the expression (x1, y1) <-> (x2, y2) 
is equal to the root of the sum of the squares of the differences between the abscissas and ordinates. 
The distance from a point to the bounding rectangle is the minimum distance from a point to this rectangle, 
or zero if the point is inside it. This value is easy to compute without traversing the child points, 
and is guaranteed to be no greater than the distance to any of the child points.

Let's look at the search algorithm for the above query.
https://mega.nz/file/9wk30YTZ#yBfUBBaQ7Lmb-y0hStVCnQhrEOLqxMH3R0Qjr_gynm4

The search starts from the root node. It has two bounding boxes. The distance to (1,1) - (6.3) is 4.0, and to (5.5) - (8.8) is 1.0.
Child nodes are traversed in order of increasing distance. 
Thus, first we go down to the nearest child node and find the distances to the points (for clarity, we will show the numbers in the figure):
https://mega.nz/file/Y5tjmYYQ#lg55qOhZDxFsNWG0pmv0gcVMh7oZ98HGVerYfgk7wxM

This information is already enough to return the first two points (5,5) and (7,8). 
Since we know that the distance to points inside the rectangle (1,1) - (6,3) is 4.0 or more, there is no need to descend to the first child node.

What if we needed to find the first three points?
postgres=# select * from points order by p <-> point '(4,7)' limit 3;
   p  
-------
 (5,5)
 (7,8)
 (8,6)
(3 rows)

Although these points are all contained in the second child, we cannot return (8,6) without looking at the first child, 
as there may be closer points (since 4.0 <4.1).
https://mega.nz/file/Vp0TEKLb#Z1DLufzrKYJ0WXc9v2M2J0bMS-NgZikMzq4yYN8cDU8

This example clarifies the requirements for the distance function for internal records. 
Choosing a smaller distance for the second record (4.0 instead of real 4.5), we worsened the efficiency 
(the algorithm in vain began to look at the extra node), but did not violate the correct operation.

Until recently, GiST was the only access method that can handle ordering operators. 
But the situation has changed: the RUM method has already been added to this company (which we will talk about later), 
and it is possible that the good old B-tree will join them - a patch written by our colleague Nikita Glukhov is being discussed by the community.

R-tree for intervals

Another example of using the gist accessor is for indexing intervals, such as time (type tsrange). 
The only difference is that the inner nodes of the tree will not contain bounding boxes, but bounding boxes.

A simple example. We will rent out a house in the village and store the booking intervals in the table:
postgres=# create table reservations(during tsrange);
CREATE TABLE
postgres=# insert into reservations(during) values
('[2016-12-30, 2017-01-09)'),
('[2017-02-23, 2017-02-27)'),
('[2017-04-29, 2017-05-02)');
INSERT 0 3
postgres=# create index on reservations using gist(during);
CREATE INDEX

The index can be used, for example, to speed up the following query:
postgres=# select * from reservations where during && '[2017-01-01, 2017-04-01)';
                    during                    
-----------------------------------------------
 ["2016-12-30 00:00:00","2017-01-08 00:00:00")
 ["2017-02-23 00:00:00","2017-02-26 00:00:00")
(2 rows)

postgres=# explain (costs off) select * from reservations where during && '[2017-01-01, 2017-04-01)';
                                     QUERY PLAN                                    
------------------------------------------------------------------------------------
 Index Only Scan using reservations_during_idx on reservations
   Index Cond: (during && '["2017-01-01 00:00:00","2017-04-01 00:00:00")'::tsrange)
(2 rows)

The && operator for intervals denotes intersection; thus, the query should return all intervals that intersect with the given one. 
For such an operator, the consistency function determines whether the specified interval intersects with a value in an internal or leaf record.

Note that in this case, too, we are not talking about getting the intervals in a certain order, although comparison operators are defined for the intervals. 
For them, you can use the b-tree index, but in this case you will have to do without supporting such operations as:

postgres=# select amop.amopopr::regoperator, amop.amoppurpose, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'range_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'gist'
and amop.amoplefttype = opc.opcintype;
         amopopr         | amoppurpose | amopstrategy
-------------------------+-------------+--------------
 @>(anyrange,anyelement) | s           |           16  содержит элемент
 <<(anyrange,anyrange)   | s           |            1  строго слева
 &<(anyrange,anyrange)   | s           |            2  не выходит за правую границу
 &&(anyrange,anyrange)   | s           |            3  пересекается
 &>(anyrange,anyrange)   | s           |            4  не выходит за левую границу
 >>(anyrange,anyrange)   | s           |            5  строго справа
 -|-(anyrange,anyrange)  | s           |            6  прилегает
 @>(anyrange,anyrange)   | s           |            7  содержит интервал
 <@(anyrange,anyrange)   | s           |            8  содержится в интервале
 =(anyrange,anyrange)    | s           |           18  равен
(10 rows)

(Except for equality, which is also part of the operator class for the btree accessor.)

Inside

You can look inside with the same gevel extension. You just need to remember to change the data type in the call to gist_print:

postgres = # select level, a from gist_print ('reservations_during_idx')
as t (level int, valid bool, a tsrange);
  level | a
------- + ------------------------------------------ -----
      1 | ["2016-12-30 00:00:00", "2017-01-09 00:00:00")
      1 | ["2017-02-23 00:00:00", "2017-02-27 00:00:00")
      1 | ["2017-04-29 00:00:00", "2017-05-02 00:00:00")
(3 rows)

Limiting exclusion

The GiST index can be used to support exclude constraints.
An exclusion constraint ensures that the specified fields of any two rows in a table do not "match" each other in the sense of some operator. 
If you choose "equal" as the operator, you get exactly the uniqueness constraint: the specified fields of any two strings are not equal to each other.
Like the unique constraint, the exclusion constraint is supported by the index. You can choose any operator, as long as it:

  1 - supported by an index method - the can_exclude property (for example, btree, gist or spgist methods, but not gin);
  2 - was commutative, that is, the condition must be met: a operator b = b operator a.
  
Here is a list of suitable strategies and examples of operators (operators, as we recall, can be called differently and are not available for all data types):

* for btree:
  * "Equal" =

* for gist and spgist:
  * "Intersection" &&
  * "Match" ~ =
  * "Fit" - | -

Note that it is possible to use the equality operator in the exclusion constraint, 
but it has no practical meaning: the uniqueness constraint will be more efficient. 
This is why we did not touch on exclusion constraints when we talked about B-trees.

Let's give an example of using an exclusion constraint. It is logical not to allow reservations of a house for overlapping time intervals.

postgres = # alter table reservations add exclude using gist (during with &&);
ALTER TABLE

After creating the integrity constraint, we can add the lines:

postgres = # insert into reservations (during) values ​​('[2017-06-10, 2017-06-13)');
INSERT 0 1

But an attempt to insert an intersecting interval into the table will result in an error:

postgres = # insert into reservations (during) values ​​('[2017-05-15, 2017-06-15)');
ERROR: conflicting key value violates exclusion constraint "reservations_during_excl"
DETAIL: Key (during) = (["2017-05-15 00:00:00", "2017-06-15 00:00:00")) 
conflicts with existing key (during) = (["2017-06 -10 00:00:00 "," 2017-06-13 00:00:00 ")).

Btree_gist extension 

Let's complicate the task. Our modest business is expanding and we are going to rent out a few houses:

postgres = # alter table reservations add house_no integer default 1;
ALTER TABLE

We need to change the exclusion limitation to include the house number. However, GiST does not support the equality operation for integers:

postgres = # alter table reservations drop constraint reservations_during_excl;
ALTER TABLE
postgres = # alter table reservations add exclude using gist (during with &&, house_no with =);
ERROR: data type integer has no default operator class for access method "gist"
HINT: You must specify an operator class for the index or define a default operator class for the data type.

In this case, the btree_gist extension will help. https://www.postgresql.org/docs/13/btree-gist.html, 
which adds GiST support for B-tree-specific operations. After all, GiST can work with any operator, so why not teach it to work with greater, less, equal?

postgres = # create extension btree_gist;
CREATE EXTENSION
postgres = # alter table reservations add exclude using gist (during with &&, house_no with =);
ALTER TABLE

Now we still cannot book the first house for the same dates:

postgres = # insert into reservations (during, house_no) values ('[2017-05-15, 2017-06-15)', 1);
ERROR: conflicting key value violates exclusion constraint "reservations_during_house_no_excl"

But we can book a second one:

postgres = # insert into reservations (during, house_no) values ('[2017-05-15, 2017-06-15)', 2);
INSERT 0 1

But you need to understand that although GiST can somehow work with the operations "greater than", "less", "equal", the B-tree still copes with them better. 
So this technique should only be used if the GiST index is essentially needed - as in our example.

RD tree for full-text search

About full-text search

Let's start with a minimalist introduction to PostgreSQL full-text search (if you're in the know, you can safely skip this section).

The task of full-text search is to select among the set of documents those that match the search query. 
(If there are many results, then it is important to find the best match, but we will not talk about this for now.)

A document for search purposes is coerced into a special type tsvector, which contains tokens and their positions in the document. 
Lexemes are words that have been converted to a searchable form. For example, by default, 
words are converted to lower case and their variable endings are cut off:

postgres = # set default_text_search_config = russian;
SET
postgres = # select to_tsvector ('And Aibolit got up, Aibolit ran. He runs through the fields, through forests, through meadows.');
                            to_tsvector
-------------------------------------------------- ------------------
 'aybol': 3.5 'beige': 13 'stood up': 2 'forest': 9 'meadow': 11 'run': 4 'floor': 7
(1 row)

It can also be seen that some words (called stop words) are omitted altogether ("and", "by"), 
since they are presumably too common for a search to be meaningful. Of course, all these transformations are customizable, but that's not the point right now.

The search query is represented by a different type - tsquery. Roughly speaking, 
a query consists of one or more tokens connected by logical connectives: “and” &, “or” |, “not”!. 
You can also use parentheses to qualify the priority of operations.

postgres = # select to_tsquery ('Aybolit & (ran | went)');
            to_tsquery
----------------------------------
 'aybol' & ('escape' | 'go')
(1 row)

For full-text searches, a single match operator @@ is used.

postgres = # select to_tsvector ('And Aibolit got up, Aibolit ran.') @@ to_tsquery ('Aibolit & (ran | went)');
 ? column?
----------
 t
(1 row)

postgres = # select to_tsvector ('And Aibolit got up, Aibolit ran.') @@ to_tsquery ('Barmaley & (ran | went)');
 ? column?
----------
 f
(1 row)

This information will be sufficient for now. We'll talk a little more about full-text search in one of the next sections on the GIN index.

RD trees

For full-text search to work quickly, you need, firstly, 
to store a column of the tsvector type in the table (so as not to perform an expensive conversion every time you search), 
and secondly, to build an index on this field. One of the possible accessors for this is GiST.

postgres = # create table ts (doc text, doc_tsv tsvector);
CREATE TABLE
postgres = # create index on ts using gist (doc_tsv);
CREATE INDEX
postgres = # insert into ts (doc) values
  ('A birch stood in the field'), ('A curly one stood in the field'), ('Lyuli, lyuli, stood'),
  ('There is no one to birch a birch'), ('There is no one to curl up'), ('Lyuli, lyuli, zalomati'),
  ('I'll go for a walk'), ('I'll break the white birch'), ('Lyuli, lyuli, I'll break it');
INSERT 0 9
postgres = # update ts set doc_tsv = to_tsvector (doc);
UPDATE 9

The last step (converting the document to a tsvector) is, of course, conveniently assigned to the trigger.

postgres = # select * from ts;
           doc                  | doc_tsv
------------------------------- + -------------------------------------
 There was a birch in the field | 'birch': 3 'floor': 2 'standing': 4
 In the field curly stood       | 'curly': 3 'floor': 2 'standing': 4
 Lyuli, Lyuli, stood            | 'lyul': 1.2 'standing': 3
 No one to break a birch        | 'birch': 2 'zalomat': 3 'neck': 1
 No one to curl up              | 'zalomat': 3 'curly': 2 'neck': 1
 Lyuli, lyuli, zalomati         | 'zalomat': 3 'lyul': 1.2
 I'll go for a walk             | 'walk': 3 'go': 2
 I will break the white birch   | 'white': 1 'birches': 2 'creases': 3
 Lyuli, Lyuli, I will break     | 'crease': 3 'lyul': 1.2
(9 rows)

How should the index be arranged? The R-tree itself is not suitable here - it is not clear what a "bounding rectangle" is for documents. 
But you can apply some modification of this approach for sets - the so-called RD-tree (RD stands for Russian Doll, matryoshka). 
By a set in this case, we mean a set of document lexemes, but in general, a set can be anything.

The idea behind RD trees is to take a bounding set instead of a bounding box — that is, a set containing all the elements of the child sets.
An important question is how to represent sets in index records. Probably the most straightforward way is to simply list all the elements of the set. 
This is how it might look:

An important question is how to represent sets in index records. Probably the most straightforward way is to simply list all the elements of the set. 
This is how it might look:
https://mega.nz/file/AokwRAIC#3Rfn09K4eTpoYNgpqzXATBsp1LqfQLAzARyrWiI6OSQ

Then for example, for access by condition doc_tsv @@ to_tsquery('sit') we could descend only to those nodes that contain "sit" lexeme:
https://mega.nz/file/QtsgWIgI#hbQW5Qdi-26AqDTme7paUeZKvyCoVKC29khCDsPrQyM

This representation has evident issues. 
The number of lexemes in a document can be pretty large, so index rows will have large size and get into TOAST, making the index far less efficient. 
Even if each document has few unique lexemes, the union of sets may still be very large: the higher to the root the larger index rows.

A representation like this is sometimes used, but for other data types. And full-text search uses another, more compact, solution — a so-called signature tree. 
Its idea is quite familiar to all who dealt with Bloom filter.

Each lexeme can be represented with its signature: a bit string of a certain length where all bits but one are zero. 
The position of this bit is determined by the value of hash function of the lexeme (we discussed internals of hash functions earlier).
The document signature is the bitwise OR of the signatures of all document lexemes.
Let's assume the following signatures of lexemes:

could    1000000
ever     0001000
good     0000010
mani     0000100
sheet    0000100
sleekest 0100000
sit      0010000
slit     0001000
slitter  0000001
upon     0000010
whoever  0010000

Then signatures of the documents are like these:

Can a sheet slitter slit sheets?                       0001101
How many sheets could a sheet slitter slit?            1001101
I slit a sheet, a sheet I slit.                        0001100
Upon a slitted sheet I sit.                            0011110
Whoever slit the sheets is a good sheet slitter.       0011111
I am a sheet slitter.                                  0000101
I slit sheets.                                         0001100
I am the sleekest sheet slitter that ever slit sheets. 0101101
She slits the sheet she sits on.                       0011100

The index tree can be represented as follows:

https://mega.nz/file/p5kCgCAY#z_Owjdm-kOVsF-vCtcjUIfdbE5Dr6v6UxSlldCeXmoo

The advantages of this approach are evident: index rows have equal small sizes, and such an index is compact. 
But a drawback is also clear: the accuracy is sacrificed to compactness.
Let's consider the same condition doc_tsv @@ to_tsquery('sit'). 
And let's compute the signature of the search query the same way as for the document: 0010000 in this case. 
The consistency function must return all child nodes whose signatures contain at least one bit from the query signature:

https://mega.nz/file/wg9g2QpS#E-0cEwjP_wjdrJt-cyAR9LdWnL0KdDao7PJoE4VA264

Compare with the figure above: we can see that the tree turned yellow, 
which means that false positives occur and excessive nodes are went through during the search. 
Here we picked up "whoever" lexeme, whose signature unfortunately was the same as the signature of "sit" lexeme. 
It's important that no false negatives can occur in the pattern, that is, we are sure not to miss needed values.

Besides, it may so happen that different documents will also get the same signatures: in our example, unlucky documents are "I slit a sheet, 
a sheet I slit" and "I slit sheets" (both have the signature of 0001100). And if a leaf index row does not store the value of "tsvector", 
the index itself will give false positives. Of course, in this case, the method will ask the indexing engine to recheck the result with the table, 
so the user will not see these false positives. But the search efficiency may get compromised.

Actually, a signature is 124-byte large in the current implementation instead of 7-bit in the figures, 
so the above issues are much less likely to occur than in the example. But in reality, much more documents get indexed as well. 
To somehow reduce the number of false positives of the index method, 
the implementation gets a little tricky: the indexed "tsvector" is stored in a leaf index row, 
but only if its size is not large (a little less than 1/16 of a page, which is about half a kilobyte for 8-KB pages).

Example
To see how indexing works on actual data, let's take the archive of "pgsql-hackers" email. 
The version used in the example contains 356125 messages with the send date, subject, author, and text:

fts=# select * from mail_messages order by sent limit 1;

-[ RECORD 1 ]------------------------------------------------------------------------
id         | 1572389
parent_id  | 1562808
sent       | 1997-06-24 11:31:09
subject    | Re: [HACKERS] Array bug is still there....
author     | "Thomas G. Lockhart" <Thomas.Lockhart@jpl.nasa.gov>
body_plain | Andrew Martin wrote:                                                    +
           | > Just run the regression tests on 6.1 and as I suspected the array bug +
           | > is still there. The regression test passes because the expected output+
           | > has been fixed to the *wrong* output.                                 +
           |                                                                         +
           | OK, I think I understand the current array behavior, which is apparently+
           | different than the behavior for v1.0x.                                  +
             ...
             
Adding and filling in the column of "tsvector" type and building the index. 
Here we will join three values in one vector (subject, author, and message text) to show that the document does not need to be one field, 
but can consist of totally different arbitrary parts.

fts=# alter table mail_messages add column tsv tsvector;

fts=# update mail_messages
set tsv = to_tsvector(subject||' '||author||' '||body_plain);

NOTICE:  word is too long to be indexed
DETAIL:  Words longer than 2047 characters are ignored.
...
UPDATE 356125

fts=# create index on mail_messages using gist(tsv); 

As we can see, a certain number of words were dropped because of too large size. But the index is eventually created and can support search queries:

fts=# explain (analyze, costs off)
select * from mail_messages where tsv @@ to_tsquery('magic & value');

                        QUERY PLAN
----------------------------------------------------------
 Index Scan using mail_messages_tsv_idx on mail_messages
 (actual time=0.998..416.335 rows=898 loops=1)
   Index Cond: (tsv @@ to_tsquery('magic & value'::text))
   Rows Removed by Index Recheck: 7859
 Planning time: 0.203 ms
 Execution time: 416.492 ms
(5 rows)
             
We can see that together with 898 rows matching the condition, the access method returned 7859 more rows that were filtered out by rechecking with the table. This demonstrates a negative impact of the loss of accuracy on the efficiency.

Internals
To analyze the contents of the index, we will again use "gevel" extension:

fts=# select level, a
from gist_print('mail_messages_tsv_idx') as t(level int, valid bool, a gtsvector)
where a is not null;

 level |               a              
-------+-------------------------------
     1 | 992 true bits, 0 false bits
     2 | 988 true bits, 4 false bits
     3 | 573 true bits, 419 false bits
     4 | 65 unique words
     4 | 107 unique words
     4 | 64 unique words
     4 | 42 unique words
...

Values of the specialized type "gtsvector" that are stored in index rows are actually the signature plus, maybe, the source "tsvector". 
If the vector is available, the output contains the number of lexemes (unique words), otherwise, the number of true and false bits in the signature.

It is clear that in the root node, the signature degenerated to "all ones", that is, 
one index level became absolutely useless (and one more became almost useless, with only four false bits).

Properties
Let's look at the properties of GiST access method (queries were provided earlier):


 amname |     name      | pg_indexam_has_property
--------+---------------+-------------------------
 gist   | can_order     | f
 gist   | can_unique    | f
 gist   | can_multi_col | t
 gist   | can_exclude   | t
Sorting of values and unique constraint are not supported. As we've seen, the index can be built on several columns and used in exclusion constraints.

The following index-layer properties are available:


     name      | pg_index_has_property
---------------+-----------------------
 clusterable   | t
 index_scan    | t
 bitmap_scan   | t
 backward_scan | f
And the most interesting properties are those of the column layer. Some of the properties are independent of operator classes:


        name        | pg_index_column_has_property
--------------------+------------------------------
 asc                | f
 desc               | f
 nulls_first        | f
 nulls_last         | f
 orderable          | f
 search_array       | f
 search_nulls       | t
(Sorting is not supported; the index cannot be used to search an array; NULLs are supported.)

But the two remaining properties, "distance_orderable" and "returnable", will depend on the operator class used. For example, for points we will get:


        name        | pg_index_column_has_property
--------------------+------------------------------
 distance_orderable | t
 returnable         | t
The first property tells that the distance operator is available for search of nearest neighbors. 
And the second one tells that the index can be used for index-only scan. Although leaf index rows store rectangles rather than points, 
the access method can return what's needed.

The following are the properties for intervals:


        name        | pg_index_column_has_property
--------------------+------------------------------
 distance_orderable | f
 returnable         | t
For intervals, the distance function is not defined and therefore, search of nearest neighbors is not possible.

And for full-text search, we get:


        name        | pg_index_column_has_property
--------------------+------------------------------
 distance_orderable | f
 returnable         | f
Support of index-only scan has been lost since leaf rows can contain only the signature without the data itself. 
However, this is a minor loss since nobody is interested in the value of type "tsvector" anyway: this value is used to select rows, 
while it is source text that needs to be shown, but is missing from the index anyway.

Other data types
Finally, we will mention a few more types that are currently supported by GiST access method in addition to already discussed geometric types 
(by example of points), intervals, and full-text search types.

Of standard types, this is the type "inet" for IP-addresses. All the rest is added through extensions:

 * cube provides "cube" data type for multi-dimensional cubes. For this type, just as for geometric types in a plane, GiST operator class is defined: R-tree, 
   supporting search for nearest neighbors.
 * seg provides "seg" data type for intervals with boundaries specified to a certain accuracy and adds support of GiST index for this data type (R-tree).
 * intarray extends the functionality of integer arrays and adds GiST support for them. 
   Two operator classes are implemented: "gist__int_ops" (RD-tree with a full representation of keys in index rows) and "gist__bigint_ops" (signature RD-tree). 
   The first class can be used for small arrays, and the second one - for larger sizes.
 * ltree adds "ltree" data type for tree-like structures and GiST support for this data type (RD-tree).
 * pg_trgm adds a specialized operator class "gist_trgm_ops" for use of trigrams in full-text search. But this is to be discussed further, along with GIN index.

-----------
- SP-GiST -
-----------

First, a few words about this name. The "GiST" part alludes to some similarity with the same-name access method. 
The similarity does exist: both are generalized search trees that provide a framework for building various access methods.

"SP" stands for space partitioning. The space here is often just what we are used to call a space, for example, a two-dimensional plane. 
But we will see that any search space is meant, that is, actually any value domain.

SP-GiST is suitable for structures where the space can be recursively split into non-intersecting areas. 
This class comprises quadtrees, k-dimensional trees (k-D trees), and radix trees.

Structure
So, the idea of SP-GiST access method is to split the value domain into non-overlapping subdomains each of which, in turn, can also be split. 
Partitioning like this induces non-balanced trees (unlike B-trees and regular GiST).

The trait of being non-intersecting simplifies decision-making during insertion and search. On the other hand, as a rule, the trees induced are of low branching. 
For example, a node of a quadtree usually has four child nodes (unlike B-trees, where the nodes amount to hundreds) and larger depth. 
Trees like these well suit the work in RAM, but the index is stored on a disk and therefore, to reduce the number of I/O operations, 
nodes have to be packed into pages, and it is not easy to do this efficiently. Besides, the time it takes to find different values in the index, 
may vary because of differences in branch depths.

This access method, same way as GiST, takes care of low-level tasks (simultaneous access and locks, logging, 
and a pure search algorithm) and provides a specialized simplified interface to enable adding support for new data types and for new partitioning algorithms.

An internal node of SP-GiST tree stores references to child nodes; a label can be defined for each reference. Besides, 
an internal node can store a value called a prefix. Actually this value is not obligatory a prefix; it can be regarded as an arbitrary predicate that is met for all child nodes.

Leaf nodes of SP-GiST contain a value of the indexed type and a reference to a table row (TID). The indexed data itself (search key) can be used as the value, 
but not obligatory: a shortened value can be stored.

In addition, leaf nodes can be grouped into lists. So, an internal node can reference not only one value, but a whole list.

Note that prefixes, labels, and values in leaf nodes have their own data types, independent of one another.

Same way as in GiST, the main function to define for search is the consistency function. 
This function is called for a tree node and returns a set of child nodes whose values "are consistent" with the search predicate 
(as usual, in the form "indexed-field operator expression"). 
For a leaf node, the consistency function determines whether the indexed value in this node meets the search predicate.

The search starts with the root node. The consistency function permits to find out which child nodes it makes sense to visit. 
The algorithm repeats for each of the nodes found. The search is depth-first.

At the physical level, index nodes are packed into pages to make work with the nodes efficient from the point of view of I/O operations. 
Note that one page can contain either internal or leaf nodes, but not both.

Example: quadtree
A quadtree is used to index points in a plane. An idea is to recursively split areas into four parts (quadrants) with respect to the central point. 
The depth of branches in such a tree can vary and depends on the density of points in appropriate quadrants.

This is what it looks like in figures, by example of the demo database augmented by airports from the site openflights.org. By the way, 
recently we released a new version of the database in which, among the rest, we replaced longitude and latitude with one field of type "point".
https://mega.nz/file/h18FyKBT#HSAj0xfHXKf31-iZn5S6D4MT2GlzzcxgZ9ooYI556-Y
First, we split the plane into four quadrants...
https://mega.nz/file/hoEU2J7T#bkEQOAiPIfxdpKThsjZbYlDTvAU1OP-exFkvreYkkXU
Then we split each of the quadrants...
https://mega.nz/file/1pNUWBwB#_Gx77DdD0hltd2eMOOXsOp0K7na8VIzVJ1Hv8KLYPpc
And so on until we get the final partitioning.

Let's provide more details of a simple example that we already considered in the GiST-related article. See what the partitioning may look like in this case:
https://mega.nz/file/dgUQ2ZCS#txb9OPujJTNfM6s7ckR-FVjQ5NCyBU705a1ACTfLOo0

The quadrants are numbered as shown in the first figure. For definiteness sake, let's place child nodes from left to right exactly in the same sequence. 
A possible index structure in this case is shown in the figure below. Each internal node references a maximum of four child nodes. 
Each reference can be labeled with the quadrant number, as in the figure. 
But there is no label in the implementation since it is more convenient to store a fixed array of four references some of which can be empty.
https://mega.nz/file/dgUQ2ZCS#txb9OPujJTNfM6s7ckR-FVjQ5NCyBU705a1ACTfLOo0
Points that lie on the boundaries relate to the quadrant with the smaller number.

postgres=# create table points(p point);

postgres=# insert into points(p) values
  (point '(1,1)'), (point '(3,2)'), (point '(6,3)'),
  (point '(5,5)'), (point '(7,8)'), (point '(8,6)');

postgres=# create index points_quad_idx on points using spgist(p);
In this case, "quad_point_ops" operator class is used by default, which contains the following operators:

postgres=# select amop.amopopr::regoperator, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'quad_point_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'spgist'
and amop.amoplefttype = opc.opcintype;

     amopopr     | amopstrategy
-----------------+--------------
 <<(point,point) |            1  strictly left
 >>(point,point) |            5  strictly right
 ~=(point,point) |            6  coincides
 <^(point,point) |           10  strictly below
 >^(point,point) |           11  strictly above
 <@(point,box)   |            8  contained in rectangle
(6 rows)

For example, let's look how the query select * from points where p >^ point '(2,7)' will be performed (find all points that lie above the given one).
https://mega.nz/file/gslRkI6a#qh9ITkbArKfZ4HFelgmJmvqodeJChuvilXxbUOdApHw

We start with the root node and use the consistency function to select to which child nodes to descend. 
For the operator >^, this function compares the point (2,7) with the central point of the node (4,4) 
and selects the quadrants that may contain the points sought, in this case, the first and fourth quadrants.

In the node corresponding to the first quadrant, we again determine the child nodes using the consistency function. 
The central point is (6,6), and we again need to look through the first and fourth quadrants.
https://mega.nz/file/8s8VwQCb#HISjN-Xdzv5BOKtYC_fcASoc7r48By3EnYJWNc7zjuQ
The list of leaf nodes (8,6) and (7,8) corresponds to the first quadrant, of which only the point (7,8) meets the query condition. 
The reference to the fourth quadrant is empty.

In the internal node (4,4), the reference to the fourth quadrant is empty as well, which completes the search.

postgres=# set enable_seqscan = off;

postgres=# explain (costs off) select * from points where p >^ point '(2,7)';

                   QUERY PLAN                  
------------------------------------------------
 Index Only Scan using points_quad_idx on points
   Index Cond: (p >^ '(2,7)'::point)
(2 rows)

Internals
We can explore the internal structure of SP-GiST indexes using "gevel" extension, which was mentioned earlier. 
Bad news is that due to a bug, this extension works incorrectly with modern versions of PostgreSQL. 
Good news is that we plan to augment "pageinspect" with the functionality of "gevel" (discussion). And the bug has already been fixed in "pageinspect".

Again, bad news is that the patch has stuck with no progress.
 
For example, let's take the extended demo database, which was used to draw pictures with the world map.

demo=# create index airports_coordinates_quad_idx on airports_ml using spgist(coordinates);
First, we can get some statistics for the index:


demo=# select * from spgist_stats('airports_coordinates_quad_idx');

           spgist_stats           
----------------------------------
 totalPages:        33           +
 deletedPages:      0            +
 innerPages:        3            +
 leafPages:         30           +
 emptyPages:        2            +
 usedSpace:         201.53 kbytes+
 usedInnerSpace:    2.17 kbytes  +
 usedLeafSpace:     199.36 kbytes+
 freeSpace:         61.44 kbytes +
 fillRatio:         76.64%       +
 leafTuples:        5993         +
 innerTuples:       37           +
 innerAllTheSame:   0            +
 leafPlaceholders:  725          +
 innerPlaceholders: 0            +
 leafRedirects:     0            +
 innerRedirects:    0
(1 row)
And second, we can output the index tree itself:

demo=# select tid, n, level, tid_ptr, prefix, leaf_value
from spgist_print('airports_coordinates_quad_idx') as t(
  tid tid,
  allthesame bool,
  n int,
  level int,
  tid_ptr tid,
  prefix point,    -- prefix type
  node_label int,  -- label type (unused here)
  leaf_value point -- list value type
)
order by tid, n;

   tid   | n | level | tid_ptr |      prefix      |    leaf_value
---------+---+-------+---------+------------------+------------------
 (1,1)   | 0 |     1 | (5,3)   | (-10.220,53.588) |
 (1,1)   | 1 |     1 | (5,2)   | (-10.220,53.588) |
 (1,1)   | 2 |     1 | (5,1)   | (-10.220,53.588) |
 (1,1)   | 3 |     1 | (5,14)  | (-10.220,53.588) |
 (3,68)  |   |     3 |         |                  | (86.107,55.270)
 (3,70)  |   |     3 |         |                  | (129.771,62.093)
 (3,85)  |   |     4 |         |                  | (57.684,-20.430)
 (3,122) |   |     4 |         |                  | (107.438,51.808)
 (3,154) |   |     3 |         |                  | (-51.678,64.191)
 (5,1)   | 0 |     2 | (24,27) | (-88.680,48.638) |
 (5,1)   | 1 |     2 | (5,7)   | (-88.680,48.638) |
 ...
But keep in mind that "spgist_print" outputs not all leaf values, but only the first one from the list, 
and therefore shows the structure of the index rather than its full contents.
Example: k-dimensional trees
For the same points in the plane, we can also suggest another way to partition the space.

Let's draw a horizontal line through the first point being indexed. It splits the plane into two parts: upper and lower. 
The second point to be indexed falls into one of these parts. Through this point, let's draw a vertical line, 
which splits this part into two ones: right and left. 
We again draw a horizontal line through the next point and a vertical line through yet the next point, and so on.

All internal nodes of the tree built this way will have only two child nodes. 
Each of the two references can lead either to the internal node that is next in the hierarchy or to the list of leaf nodes.

This method can be easily generalized for k-dimensional spaces, and therefore, the trees are also called k-dimensional (k-D trees) in the literature.

Explaining the method by example of airports:
https://mega.nz/file/skkURAbI#xc_qsFn4_GdkxWN1-XMdj57v9UVzf_nz2LUkiLZmXas
First we split the plane into upper and lower parts...
https://mega.nz/file/gx9S2KQJ#LXq3IX8jjmyX6fAMXP9DQDcSkWYQTviak76ETMoyqW4
Then we split each part into left and right parts...
https://mega.nz/file/ggsC3KwI#jGKvetZuJStqHkdV_TiQYSppMjOE9vvNSHcRCgg4qMQ
And so on until we get the final partitioning.

To use a partitioning just like this, we need to explicitly specify the operator class "kd_point_ops" when creating an index.

postgres=# create index points_kd_idx on points using spgist(p kd_point_ops);
This class includes exactly the same operators as the "default" class "quad_point_ops".

Internals

When looking through the tree structure, we need to take into account that the prefix in this case is only one coordinate rather than a point:

demo=# select tid, n, level, tid_ptr, prefix, leaf_value
from spgist_print('airports_coordinates_kd_idx') as t(
  tid tid,
  allthesame bool,
  n int,
  level int,
  tid_ptr tid,
  prefix float,    -- prefix type
  node_label int,  -- label type (unused here)
  leaf_value point -- list node type
)
order by tid, n;

   tid   | n | level | tid_ptr |   prefix   |    leaf_value
---------+---+-------+---------+------------+------------------
 (1,1)   | 0 |     1 | (5,1)   |     53.740 |
 (1,1)   | 1 |     1 | (5,4)   |     53.740 |
 (3,113) |   |     6 |         |            | (-7.277,62.064)
 (3,114) |   |     6 |         |            | (-85.033,73.006)
 (5,1)   | 0 |     2 | (5,12)  |    -65.449 |
 (5,1)   | 1 |     2 | (5,2)   |    -65.449 |
 (5,2)   | 0 |     3 | (5,6)   |     35.624 |
 (5,2)   | 1 |     3 | (5,3)   |     35.624 |
 ...
 
Example: radix tree

We can also use SP-GiST to implement a radix tree for strings. 
The idea of a radix tree is that a string to be indexed is not fully stored in a leaf node, but is obtained by concatenating the values stored in the 
nodes above this one up to the root.

Assume, we need to index site URLs: "postgrespro.ru", "postgrespro.com", "postgresql.org", and "planet.postgresql.org".


postgres=# create table sites(url text);

postgres=# insert into sites values ('postgrespro.ru'),('postgrespro.com'),('postgresql.org'),('planet.postgresql.org');

postgres=# create index on sites using spgist(url);
The tree will look as follows:
https://mega.nz/file/AlsAGQ4Z#vubiqcIPTDoqsnIwYM8g1rdtjCqlCf8AeojqTf_RYa4

The internal nodes of the tree store prefixes common to all child nodes. For example, in child nodes of "stgres", the values start with "p" + "o" + "stgres".
Unlike in quadtrees, each pointer to a child node is additionally labeled with one character (more exactly, with two bytes, but this is not so important).
"text_ops" operator class supports B-tree-like operators: "equal", "greater", and "less":

postgres=# select amop.amopopr::regoperator, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'text_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'spgist'
and amop.amoplefttype = opc.opcintype;

     amopopr     | amopstrategy
-----------------+--------------
 ~<~(text,text)  |            1
 ~<=~(text,text) |            2
 =(text,text)    |            3
 ~>=~(text,text) |            4
 ~>~(text,text)  |            5
 <(text,text)    |           11
 <=(text,text)   |           12
 >=(text,text)   |           14
 >(text,text)    |           15
(9 rows)
The distinction of operators with tildes is that they manipulate bytes rather than characters.

Sometimes, a representation in the form of a radix tree may turn out to be much more compact than B-tree since the values are not fully stored, but reconstructed as the need arises while descending through the tree.

Consider a query: select * from sites where url like 'postgresp%ru'. It can be performed using the index:


postgres=# explain (costs off) select * from sites where url like 'postgresp%ru';

                                  QUERY PLAN                                  
------------------------------------------------------------------------------
 Index Only Scan using sites_url_idx on sites
   Index Cond: ((url ~>=~ 'postgresp'::text) AND (url ~<~ 'postgresq'::text))
   Filter: (url ~~ 'postgresp%ru'::text)
(3 rows)

Actually, the index is used to find values that are greater or equal to "postgresp", but less than "postgresq" (Index Cond), 
and then matching values are chosen from the result (Filter).
First, the consistency function must decide to which child nodes of "p" root we need to descend. 
Two options are available: "p" + "l" (no need to descend, which is clear even without diving deeper) and "p" + "o" + "stgres" (continue the descent).
For "stgres" node, a call to the consistency function is needed again to check "postgres" + "p" + "ro." (continue the descent) and "postgres" + "q" (no need to descend).
For "ro." node and all its child leaf nodes, the consistency function will respond "yes", 
so the index method will return two values: "postgrespro.com" and "postgrespro.ru". One matching value will be selected of them at the filtering stage.
https://mega.nz/file/w5823aTQ#2NQQrXlXWKVOHjk7NqeHWIZXpdIMRrlOuH4Tpm3VSiQ

Internals

When looking through the tree structure, we need to take data types into account:

postgres=# select * from spgist_print('sites_url_idx') as t(
  tid tid,
  allthesame bool,
  n int,
  level int,
  tid_ptr tid,
  prefix text,         -- prefix type
  node_label smallint, -- label type
  leaf_value text      -- leaf node type
)
order by tid, n;
Properties
Let's look at the properties of SP-GiST access method (queries were provided earlier):


 amname |     name      | pg_indexam_has_property
--------+---------------+-------------------------
 spgist | can_order     | f
 spgist | can_unique    | f
 spgist | can_multi_col | f
 spgist | can_exclude   | t
 
SP-GiST indexes cannot be used for sorting and for support of the unique constraint. Additionally, 
indexes like this cannot be created on several columns (unlike GiST). But it is permitted to use such indexes to support exclusion constraints.

The following index-layer properties are available:

     name      | pg_index_has_property
---------------+-----------------------
 clusterable   | f
 index_scan    | t
 bitmap_scan   | t
 backward_scan | f
The difference from GiST here is that clustering is impossible.

And eventually the following are column-layer properties:


        name        | pg_index_column_has_property 
--------------------+------------------------------
 asc                | f
 desc               | f
 nulls_first        | f
 nulls_last         | f
 orderable          | f
 distance_orderable | f
 returnable         | t
 search_array       | f
 search_nulls       | t
Sorting is not supported, which is predictable. Distance operators for search of nearest neighbors are not available in SP-GiST so far. 
Most likely, this feature will be supported in future.

It is supported in upcoming PostgreSQL 12, the patch by Nikita Glukhov.
 

SP-GiST can be used for index-only scan, at least for the discussed operator classes. As we have seen, in some instances, 
indexed values are explicitly stored in leaf nodes, while in the other ones, the values are reconstructed part by part during the tree descent.

NULLs

Not to complicate the picture, we haven't mentioned NULLs so far. It is clear from the index properties that NULLs are supported. Really:

postgres=# explain (costs off)
select * from sites where url is null;

                  QUERY PLAN                  
----------------------------------------------
 Index Only Scan using sites_url_idx on sites
   Index Cond: (url IS NULL)
(2 rows)
However, NULL is something foreign for SP-GiST. All operators from "spgist" operator class must be strict: 
an operator must return NULL whenever any of its parameters is NULL. The method itself ensures this: NULLs are just not passed to operators.

But to use the access method for index-only scan, NULLs must be stored in the index anyway. And they are stored, but in a separate tree with its own root.

Other data types

In addition to points and radix trees for strings, other methods based on SP-GiST are also implemented PostgreSQL:

"box_ops" operator class provides a quadtree for rectangles. Each rectangle is represented by a point in a four-dimensional space, 
so the number of quadrants equals 16. An index like this can beat GiST in performance when there are a lot of intersections of the rectangles: 
in GiST it is impossible to draw boundaries so as to separate intersecting objects from one another, while there are no such issues with points 
(even four-dimensional).
"range_ops" operator class provides a quadtree for intervals. An interval is represented by a two-dimensional point: 
the lower boundary becomes the abscissa, and the upper boundary becomes the ordinate.

-------
- GIN -
-------

"Gin?.. Gin is, it seems, such an American liquor?.."
"I'm not a drink, oh, inquisitive boy!" again the old man flared up, again he realized himself and again took himself in hand. 
"I am not a drink, but a powerful and undaunted spirit, and there is no such magic in the world that I would not be able to do."
— Lazar Lagin, "Old Khottabych".

Gin stands for Generalized Inverted Index and should be considered as a genie, not a drink.
— README

General concept
GIN is the abbreviated Generalized Inverted Index. This is a so-called inverted index. 
It manipulates data types whose values are not atomic, but consist of elements. We will call these types compound. 
And these are not the values that get indexed, but individual elements; each element references the values in which it occurs.

A good analogy to this method is the index at the end of a book, which for each term, provides a list of pages where this term occurs. 
The access method must ensure fast search of indexed elements, just like the index in a book. 
Therefore, these elements are stored as a familiar B-tree (a different, simpler, implementation is used for it, but it does not matter in this case). 
An ordered set of references to table rows that contain compound values with the element is linked to each element. 
Orderliness is inessential for data retrieval (the sort order of TIDs does not mean much), but important for the internal structure of the index.

Elements are never deleted from GIN index. It is taken to be that values containing elements can disappear, arise, or vary, 
but the set of elements of which they are composed is more or less stable. 
This solution considerably simplifies algorithms for concurrent work of several processes with the index.

If the list of TIDs is pretty small, it can fit into the same page as the element (and is called "the posting list"). 
But if the list is large, a more efficient data structure is needed, and we are already aware of it — it is B-tree again. 
Such a tree is located on separate data pages (and is called "the posting tree").

So, GIN index consists of the B-tree of elements, and B-trees or flat lists of TIDs are linked to leaf rows of that B-tree.

Just like GiST and SP-GiST indexes, discussed earlier, 
GIN provides an application developer with the interface to support various operations over compound data types.

Full-text search
The main application area for GIN method is speeding up full-text search, which is, therefore, 
reasonable to be used as an example in a more detailed discussion of this index.

The article related to GiST has already provided a small introduction to full-text search, so let's get straight to the point without repetitions. 
It is clear that compound values in this case are documents, and elements of these documents are lexemes.

Let's consider the example from GiST-related article:

postgres=# create table ts(doc text, doc_tsv tsvector);

postgres=# insert into ts(doc) values
  ('Can a sheet slitter slit sheets?'), 
  ('How many sheets could a sheet slitter slit?'),
  ('I slit a sheet, a sheet I slit.'),
  ('Upon a slitted sheet I sit.'), 
  ('Whoever slit the sheets is a good sheet slitter.'), 
  ('I am a sheet slitter.'),
  ('I slit sheets.'),
  ('I am the sleekest sheet slitter that ever slit sheets.'),
  ('She slits the sheet she sits on.');

postgres=# update ts set doc_tsv = to_tsvector(doc);

postgres=# create index on ts using gin(doc_tsv);
A possible structure of this index is shown in the figure:
https://mega.nz/file/ElkklbgJ#YBDZjSVQeTiib9LNIxK5dq5pQYvE6XTlkmJ80RA6y70

Unlike in all the previous figures, references to table rows (TIDs) are denoted with numeric values on a dark background 
(the page number and position on the page) rather than with arrows.

postgres=# select ctid, left(doc,20), doc_tsv from ts;

  ctid |         left         |                         doc_tsv                         
-------+----------------------+---------------------------------------------------------
 (0,1) | Can a sheet slitter  | 'sheet':3,6 'slit':5 'slitter':4
 (0,2) | How many sheets coul | 'could':4 'mani':2 'sheet':3,6 'slit':8 'slitter':7
 (0,3) | I slit a sheet, a sh | 'sheet':4,6 'slit':2,8
 (1,1) | Upon a slitted sheet | 'sheet':4 'sit':6 'slit':3 'upon':1
 (1,2) | Whoever slit the she | 'good':7 'sheet':4,8 'slit':2 'slitter':9 'whoever':1
 (1,3) | I am a sheet slitter | 'sheet':4 'slitter':5
 (2,1) | I slit sheets.       | 'sheet':3 'slit':2
 (2,2) | I am the sleekest sh | 'ever':8 'sheet':5,10 'sleekest':4 'slit':9 'slitter':6
 (2,3) | She slits the sheet  | 'sheet':4 'sit':6 'slit':2
(9 rows)
In this speculative example, the list of TIDs fit into regular pages for all lexemes but "sheet", "slit", and "slitter". 
These lexemes occurred in many documents, and the lists of TIDs for them have been placed into individual B-trees.

By the way, how can we figure out how many documents contain a lexeme? For a small table, a "direct" technique, shown below, will work, 
but we will learn further what to do with larger ones.


postgres=# select (unnest(doc_tsv)).lexeme, count(*) from ts
group by 1 order by 2 desc;

  lexeme  | count 
----------+-------
 sheet    |     9
 slit     |     8
 slitter  |     5
 sit      |     2
 upon     |     1
 mani     |     1
 whoever  |     1
 sleekest |     1
 good     |     1
 could    |     1
 ever     |     1
(11 rows)
Note also that unlike a regular B-tree, pages of GIN index are connected by a unidirectional list rather than a bidirectional one. 
This is sufficient since a tree traverse is done only one way.

Example of a query
How will the following query be performed for our example?


postgres=# explain(costs off)
select doc from ts where doc_tsv @@ to_tsquery('many & slitter');

                             QUERY PLAN                              
---------------------------------------------------------------------
 Bitmap Heap Scan on ts
   Recheck Cond: (doc_tsv @@ to_tsquery('many & slitter'::text))
   ->  Bitmap Index Scan on ts_doc_tsv_idx
         Index Cond: (doc_tsv @@ to_tsquery('many & slitter'::text))
(4 rows)
Individual lexemes (search keys) are extracted from the query first: "mani" and "slitter". 
This is done by a specialized API function that takes into account the data type and strategy determined by the operator class:


postgres=# select amop.amopopr::regoperator, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'tsvector_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'gin'
and amop.amoplefttype = opc.opcintype;

        amopopr        | amopstrategy 
-----------------------+--------------
 @@(tsvector,tsquery)  |            1  matching search query
 @@@(tsvector,tsquery) |            2  synonym for @@ (for backward compatibility)
(2 rows)
In the B-tree of lexemes, we next find both keys and go through ready lists of TIDs. We get:

for "mani" — (0,2).
for "slitter" — (0,1), (0,2), (1,2), (1,3), (2,2).
https://mega.nz/file/Bt1CTDSJ#HvVP3hXIBr2viVJi51X5pPvKYpsj-g8SJOh2GGztmio

Finally, for each TID found, an API consistency function is called, which must determine which of the rows found match the search query. 
Since the lexemes in our query are joined by Boolean "and", the only row returned is (0,2):


       |      |         |  consistency
       |      |         |    function
  TID  | mani | slitter | slit & slitter
-------+------+---------+----------------
 (0,1) |    f |       T |              f 
 (0,2) |    T |       T |              T
 (1,2) |    f |       T |              f
 (1,3) |    f |       T |              f
 (2,2) |    f |       T |              f
And the result is:


postgres=# select doc from ts where doc_tsv @@ to_tsquery('many & slitter');

                     doc                     
---------------------------------------------
 How many sheets could a sheet slitter slit?
(1 row)
If we compare this approach with the one already discussed for GiST, the advantage of GIN for full-text search appears evident. 
But there is more in this than meets the eye.

The issue of a slow update
The thing is that data insertion or update in GIN index is pretty slow. Each document usually contains many lexemes to be indexed. 
Therefore, when only one document is added or updated, we have to massively update the index tree.

On the other hand, if several documents are simultaneously updated, some of their lexemes may be the same, 
and the total amount of work will be smaller than when updating documents one by one.

GIN index has "fastupdate" storage parameter, which we can specify during index creation and update later:


postgres=# create index on ts using gin(doc_tsv) with (fastupdate = true);
With this parameter turned on, updates will be accumulated in a separate unordered list (on individual connected pages). 
When this list gets large enough or during vacuuming, all accumulated updates are instantaneously made to the index. 
What list to consider "large enough" is determined by "gin_pending_list_limit" configuration parameter or by the same-name storage parameter of the index.

But this approach has drawbacks: first, search is slowed down (since the unordered list needs to be looked through in addition to the tree), 
and second, a next update can unexpectedly take much time if the unordered list has been overflowed.

Search of a partial match
We can use partial match in full-text search. For example, consider the following query:

gin=# select doc from ts where doc_tsv @@ to_tsquery('slit:*');

                          doc                           
--------------------------------------------------------
 Can a sheet slitter slit sheets?
 How many sheets could a sheet slitter slit?
 I slit a sheet, a sheet I slit.
 Upon a slitted sheet I sit.
 Whoever slit the sheets is a good sheet slitter.
 I am a sheet slitter.
 I slit sheets.
 I am the sleekest sheet slitter that ever slit sheets.
 She slits the sheet she sits on.
(9 rows)
This query will find documents that contain lexemes starting with "slit". In this example, such lexemes are "slit" and "slitter".

A query will certainly work anyway, even without indexes, but GIN also permits to speed up the following search:

postgres=# explain (costs off)
select doc from ts where doc_tsv @@ to_tsquery('slit:*');

                         QUERY PLAN                          
-------------------------------------------------------------
 Bitmap Heap Scan on ts
   Recheck Cond: (doc_tsv @@ to_tsquery('slit:*'::text))
   ->  Bitmap Index Scan on ts_doc_tsv_idx
         Index Cond: (doc_tsv @@ to_tsquery('slit:*'::text))
(4 rows)
Here all lexemes having the prefix specified in the search query are looked up in the tree and joined by Boolean "or".

Frequent and infrequent lexemes
To watch how the indexing works on live data, let's take the archive of "pgsql-hackers" email, which we've already used while discussing GiST. 
This version of the archive contains 356125 messages with the send date, subject, author, and text.


fts=# alter table mail_messages add column tsv tsvector;

fts=# update mail_messages set tsv = to_tsvector(body_plain);

NOTICE:  word is too long to be indexed
DETAIL:  Words longer than 2047 characters are ignored.
...
UPDATE 356125

fts=# create index on mail_messages using gin(tsv);
Let's consider a lexeme that occurs in many documents. The query using "unnest" will fail to work on such a large data size, 
and the correct technique is to use "ts_stat" function, which provides the information on lexemes, the number of documents where they occurred, 
and total number of occurrences.

fts=# select word, ndoc
from ts_stat('select tsv from mail_messages')
order by ndoc desc limit 3;

 word  |  ndoc  
-------+--------
 re    | 322141
 wrote | 231174
 use   | 176917
(3 rows)
Let's choose "wrote".

And we will take some word that is infrequent for developers' email, say, "tattoo":

fts=# select word, ndoc from ts_stat('select tsv from mail_messages') where word = 'tattoo';

  word  | ndoc 
--------+------
 tattoo |    2
(1 row)
Are there any documents where both these lexemes occur? It appears that there are:


fts=# select count(*) from mail_messages where tsv @@ to_tsquery('wrote & tattoo');

 count 
-------
     1
(1 row)

A question arises how to perform this query. If we get lists of TIDs for both lexemes, as described above, the search will evidently be inefficient: 
we will have to go through more than 200 thousand values, only one of which will be left. 
Fortunately, using the planner statistics, the algorithm understands that "wrote" lexeme occurs frequently, while "tatoo" occurs infrequently. 
Therefore, the search of the infrequent lexeme is performed, and the two documents retrieved are then checked for occurrence of "wrote" lexeme. 
And this is clear from the query, which is performed quickly:

fts=# \timing on

fts=# select count(*) from mail_messages where tsv @@ to_tsquery('wrote & tattoo');

 count 
-------
     1
(1 row)
Time: 0,959 ms
The search of "wrote" alone takes considerably longer:


fts=# select count(*) from mail_messages where tsv @@ to_tsquery('wrote');

 count  
--------
 231174
(1 row)
Time: 2875,543 ms (00:02,876)
This optimization certainly works not only for two lexemes, but in more complex cases too.

Limiting the query result
A feature of GIN access method is that the result is always returned as a bitmap: this method cannot return the result TID by TID. 
It's because of this, all query plans in this article use bitmap scan.

Therefore, limitation of the index scan result using LIMIT clause is not quite efficient. 
Pay attention to the predicted cost of the operation ("cost" field of "Limit" node):

fts=# explain (costs off)
select * from mail_messages where tsv @@ to_tsquery('wrote') limit 1;

                                       QUERY PLAN
-----------------------------------------------------------------------------------------
 Limit  (cost=1283.61..1285.13 rows=1)
   ->  Bitmap Heap Scan on mail_messages  (cost=1283.61..209975.49 rows=137207)
         Recheck Cond: (tsv @@ to_tsquery('wrote'::text))
         ->  Bitmap Index Scan on mail_messages_tsv_idx  (cost=0.00..1249.30 rows=137207)
               Index Cond: (tsv @@ to_tsquery('wrote'::text))
(5 rows)
The cost is estimated as 1285.13, which is a little bit larger than the cost of building the whole bitmap 1249.30 ("cost" field of Bitmap Index Scan node).

Therefore, the index has a special capability to limit the number of results. The threshold value is specified in "gin_fuzzy_search_limit" 
configuration parameter and is equal to zero by default (no limitation takes place). But we can set the threshold value:


fts=# set gin_fuzzy_search_limit = 1000;

fts=# select count(*) from mail_messages where tsv @@ to_tsquery('wrote');

 count 
-------
  5746
(1 row)

fts=# set gin_fuzzy_search_limit = 10000;

fts=# select count(*) from mail_messages where tsv @@ to_tsquery('wrote');

 count 
-------
 14726
(1 row)
As we can see, the number of rows returned by the query differs for different parameter values (if index access is used). 
The limitation is not strict: more rows than specified can be returned, which justifies "fuzzy" part of the parameter name.

Compact representation
Among the rest, GIN indexes are good thanks to their compactness. First, if one and the same lexeme occurs in several documents (and this is usually the case), 
it is stored in the index only once. Second, TIDs are stored in the index in an ordered fashion, and this enables us to use a simple compression: 
each next TID in the list is actually stored as its difference from the previous one; this is usually a small number, 
which requires much fewer bits that a complete six-byte TID.

To get an idea of the size, let's build B-tree from the text of the messages. But a fair comparison is not certainly going to happen:

GIN is built a on different data type ("tsvector" rather than "text"), which is smaller,
at the same time, the size of messages for B-tree has to be shortened to approximately two kilobytes.
Nevertheless, we continue:

fts=# create index mail_messages_btree on mail_messages(substring(body_plain for 2048));
We will build GiST index as well:

fts=# create index mail_messages_gist on mail_messages using gist(tsv);
The size of indexes upon "vacuum full":

fts=# select pg_size_pretty(pg_relation_size('mail_messages_tsv_idx')) as gin,
             pg_size_pretty(pg_relation_size('mail_messages_gist')) as gist,
             pg_size_pretty(pg_relation_size('mail_messages_btree')) as btree;

  gin   |  gist  | btree  
--------+--------+--------
 179 MB | 125 MB | 546 MB
(1 row)
Because of representation compactness, we can try to use GIN index during migration from Oracle as a replacement for bitmap indexes 
(without going into details, I provide a reference to Lewis's post for inquisitive minds). 
As a rule, bitmap indexes are used for fields that have few unique values, which is excellent also for GIN. And, as shown in the first article, 
PostgreSQL can build a bitmap based on any index, including GIN, on the fly.

GiST or GIN?
For many data types, operator classes are available for both GiST and GIN, which raises a question which index to use. 
Perhaps, we can already make some conclusions.

As a rule, GIN beats GiST in accuracy and search speed. If the data is updated not frequently and fast search is needed, most likely GIN will be an option.

On the other hand, if the data is intensively updated, overhead costs of updating GIN may appear to be too large. 
In this case, we will have to compare both options and choose the one whose characteristics are better balanced.

Arrays

Another example of using GIN is indexing of arrays. In this case, array elements get into the index, 
which permits to speed up a number of operations over arrays:


postgres=# select amop.amopopr::regoperator, amop.amopstrategy
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'array_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'gin'
and amop.amoplefttype = opc.opcintype;

        amopopr        | amopstrategy 
-----------------------+--------------
 &&(anyarray,anyarray) |            1  intersection
 @>(anyarray,anyarray) |            2  contains array
 <@(anyarray,anyarray) |            3  contained in array
 =(anyarray,anyarray)  |            4  equality
(4 rows)

Our demo database has "routes" view with information on flights. Among the rest, this view contains "days_of_week" column - an array of weekdays 
when flights take place. For example, a flight from Vnukovo to Gelendzhik leaves on Tuesdays, Thursdays, and Sundays:

demo=# select departure_airport_name, arrival_airport_name, days_of_week
from routes
where flight_no = 'PG0049';

 departure_airport_name | arrival_airport_name | days_of_week 
------------------------+----------------------+--------------
 Vnukovo                | Gelendzhik            | {2,4,7}
(1 row)
To build the index, let's "materialize" the view into a table:

demo=# create table routes_t as select * from routes;

demo=# create index on routes_t using gin(days_of_week);
Now we can use the index to get to know all flights that leave on Tuesdays, Thursdays, and Sundays:

demo=# explain (costs off) select * from routes_t where days_of_week = ARRAY[2,4,7];

                        QUERY PLAN                         
-----------------------------------------------------------
 Bitmap Heap Scan on routes_t
   Recheck Cond: (days_of_week = '{2,4,7}'::integer[])
   ->  Bitmap Index Scan on routes_t_days_of_week_idx
         Index Cond: (days_of_week = '{2,4,7}'::integer[])
(4 rows)
It appears that there are six of them:


demo=# select flight_no, departure_airport_name, arrival_airport_name, days_of_week from routes_t where days_of_week = ARRAY[2,4,7];

 flight_no | departure_airport_name | arrival_airport_name | days_of_week 
-----------+------------------------+----------------------+--------------
 PG0005    | Domodedovo             | Pskov                | {2,4,7}
 PG0049    | Vnukovo                | Gelendzhik           | {2,4,7}
 PG0113    | Naryan-Mar             | Domodedovo           | {2,4,7}
 PG0249    | Domodedovo             | Gelendzhik           | {2,4,7}
 PG0449    | Stavropol              | Vnukovo              | {2,4,7}
 PG0540    | Barnaul                | Vnukovo              | {2,4,7}
(6 rows)
How is this query performed? Exactly the same way as described above:

From the array {2,4,7}, which plays the role of the search query here, elements (search keywords) are extracted. Evidently, 
these are the values of "2", "4", and "7".
In the tree of elements, the extracted keys are found, and for each of them the list of TIDs is selected.
Of all TIDs found, the consistency function selects those that match the operator from the query. For = operator, 
only those TIDs match it that occurred in all the three lists (in other words, the initial array must contain all the elements). 
But this is not sufficient: it is also needed for the array not to contain any other values, and we cannot check this condition with the index. 
Therefore, in this case, the access method asks the indexing engine to recheck all TIDs returned with the table.
Interestingly, there are strategies (for example, "contained in array") that cannot check anything and have to recheck all the TIDs found with the table.

But what to do if we need to know the flights that leave from Moscow on Tuesdays, Thursdays, and Sundays? The index will not support the additional condition, 
which will get into "Filter" column.


demo=# explain (costs off)
select * from routes_t where days_of_week = ARRAY[2,4,7] and departure_city = 'Moscow';

                        QUERY PLAN                         
-----------------------------------------------------------
 Bitmap Heap Scan on routes_t
   Recheck Cond: (days_of_week = '{2,4,7}'::integer[])
   Filter: (departure_city = 'Moscow'::text)
   ->  Bitmap Index Scan on routes_t_days_of_week_idx
         Index Cond: (days_of_week = '{2,4,7}'::integer[])
(5 rows)
Here this is OK (the index selects only six rows anyway), but in cases where the additional condition increases the selective capability, 
it is desired to have such a support. However, we cannot just create the index:

demo=# create index on routes_t using gin(days_of_week,departure_city);

ERROR:  data type text has no default operator class for access method "gin"
HINT:  You must specify an operator class for the index or define a default operator class for the data type.
But "btree_gin" extension will help, which adds GIN operator classes that simulate work of a regular B-tree.

demo=# create extension btree_gin;

demo=# create index on routes_t using gin(days_of_week,departure_city);

demo=# explain (costs off)
select * from routes_t where days_of_week = ARRAY[2,4,7] and departure_city = 'Moscow';

                             QUERY PLAN
---------------------------------------------------------------------
 Bitmap Heap Scan on routes_t
   Recheck Cond: ((days_of_week = '{2,4,7}'::integer[]) AND
                  (departure_city = 'Moscow'::text))
   ->  Bitmap Index Scan on routes_t_days_of_week_departure_city_idx
         Index Cond: ((days_of_week = '{2,4,7}'::integer[]) AND
                      (departure_city = 'Moscow'::text))
(4 rows)
JSONB
One more example of a compound data type that has built-in GIN support is JSON. To work with JSON values, 
a number of operators and functions are defined at present, some of which can be sped up using indexes:


postgres=# select opc.opcname, amop.amopopr::regoperator, amop.amopstrategy as str
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname in ('jsonb_ops','jsonb_path_ops')
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'gin'
and amop.amoplefttype = opc.opcintype;

    opcname     |     amopopr      | str
----------------+------------------+-----
 jsonb_ops      | ?(jsonb,text)    |   9  top-level key exists
 jsonb_ops      | ?|(jsonb,text[]) |  10  some top-level key exists
 jsonb_ops      | ?&(jsonb,text[]) |  11  all top-level keys exist
 jsonb_ops      | @>(jsonb,jsonb)  |   7  JSON value is at top level
 jsonb_path_ops | @>(jsonb,jsonb)  |   7
(5 rows)
As we can see, two operator classes are available: "jsonb_ops" and "jsonb_path_ops".

The first operator class "jsonb_ops" is used by default. All keys, values, and array elements get to the index as elements of the initial JSON document. 
An attribute is added to each of these elements, which indicates whether this element is a key (this is needed for "exists" strategies, 
which distinguish between keys and values).

For example, let's represent a few rows from "routes" as JSON as follows:


demo=# create table routes_jsonb as
  select to_jsonb(t) route 
  from (
      select departure_airport_name, arrival_airport_name, days_of_week
      from routes 
      order by flight_no limit 4
  ) t;

demo=# select ctid, jsonb_pretty(route) from routes_jsonb;

 ctid  |                 jsonb_pretty                  
-------+-------------------------------------------------
 (0,1) | {                                              +
       |     "days_of_week": [                          +
       |         1                                      +
       |     ],                                         +
       |     "arrival_airport_name": "Surgut",          +
       |     "departure_airport_name": "Ust-Ilimsk"     +
       | }
 (0,2) | {                                              +
       |     "days_of_week": [                          +
       |         2                                      +
       |     ],                                         +
       |     "arrival_airport_name": "Ust-Ilimsk",      +
       |     "departure_airport_name": "Surgut"         +
       | }
 (0,3) | {                                              +
       |     "days_of_week": [                          +
       |         1,                                     +
       |         4                                      +
       |     ],                                         +
       |     "arrival_airport_name": "Sochi",           +
       |     "departure_airport_name": "Ivanovo-Yuzhnyi"+
       | }
 (0,4) | {                                              +
       |     "days_of_week": [                          +
       |         2,                                     +
       |         5                                      +
       |     ],                                         +
       |     "arrival_airport_name": "Ivanovo-Yuzhnyi", +
       |     "departure_airport_name": "Sochi"          +
       | }
(4 rows)

demo=# create index on routes_jsonb using gin(route);
The index may look as follows:
https://mega.nz/file/58tGjZ7J#gGhBLp8k_nCn6pAlmXP6f7VYERAitG93wqzC1I70BDY

Now, a query like this, for example, may be performed using the index:

demo=# explain (costs off) 
select jsonb_pretty(route) 
from routes_jsonb 
where route @> '{"days_of_week": [5]}';

                          QUERY PLAN                           
---------------------------------------------------------------
 Bitmap Heap Scan on routes_jsonb
   Recheck Cond: (route @> '{"days_of_week": [5]}'::jsonb)
   ->  Bitmap Index Scan on routes_jsonb_route_idx
         Index Cond: (route @> '{"days_of_week": [5]}'::jsonb)
(4 rows)
Starting with the root of the JSON document, @> operator checks whether the specified route ("days_of_week": [5]) occurs. Here the query will return one row:


demo=# select jsonb_pretty(route) from routes_jsonb where route @> '{"days_of_week": [5]}';

                 jsonb_pretty                 
------------------------------------------------
 {                                             +
     "days_of_week": [                         +
         2,                                    +
         5                                     +
     ],                                        +
     "arrival_airport_name": "Ivanovo-Yuzhnyi",+
     "departure_airport_name": "Sochi"         +
 }
(1 row)

The query is performed as follows:

In the search query ("days_of_week": [5]) elements (search keys) are extracted: "days_of_week" and "5".
In the tree of elements the extracted keys are found, and for each of them the list of TIDs is selected: for "5" — (0,4), 
and for "days_of_week" — (0,1), (0,2), (0,3), (0,4).
Of all TIDs found, the consistency function selects those that match the operator from the query. For @> operator, 
documents that contain not all elements from the search query won't do for sure, so only (0,4) is left. 
But we still need to recheck the TID left with the table since it is unclear from the index in what order the found elements occur in JSON document.
To discover more details of other operators, you can read the documentation.

In addition to conventional operations for dealing with JSON, "jsquery" extension has long been available, 
which defines a query language with richer capabilities (and certainly, with support of GIN indexes). 
Besides, in 2016, a new SQL standard was issued, which defines its own set of operations and query language "SQL/JSON path". 
An implementation of this standard has already been accomplished, and we believe it will appear in PostgreSQL 11.

The SQL/JSON path patch was finally committed to PostgreSQL 12, while other pieces are still on the way. 
Hopefully we'll see the fully implemented feature in PostgreSQL 13.
 
Internals

We can look inside GIN index using "pageinspect" extension.


fts=# create extension pageinspect;
The information from the meta page shows general statistics:

fts=# select * from gin_metapage_info(get_raw_page('mail_messages_tsv_idx',0));

-[ RECORD 1 ]----+-----------
pending_head     | 4294967295
pending_tail     | 4294967295
tail_free_size   | 0
n_pending_pages  | 0
n_pending_tuples | 0
n_total_pages    | 22968
n_entry_pages    | 13751
n_data_pages     | 9216
n_entries        | 1423598
version          | 2
The page structure provides a special area where access methods store their information; this area is "opaque" for ordinary programs such as vacuum. 
"gin_page_opaque_info" function shows this data for GIN. For example, we can get to know the set of index pages:

fts=# select flags, count(*)
from generate_series(1,22967) as g(id), -- n_total_pages
     gin_page_opaque_info(get_raw_page('mail_messages_tsv_idx',g.id))
group by flags;

         flags          | count 
------------------------+-------
 {meta}                 |     1  meta page
 {}                     |   133  internal page of element B-tree
 {leaf}                 | 13618  leaf page of element B-tree
 {data}                 |  1497  internal page of TID B-tree
 {data,leaf,compressed} |  7719  leaf page of TID B-tree
(5 rows)
"gin_leafpage_items" function provides information on TIDs stored on pages {data,leaf,compressed}:

fts=# select * from gin_leafpage_items(get_raw_page('mail_messages_tsv_idx',2672));

-[ RECORD 1 ]---------------------------------------------------------------------
first_tid | (239,44)
nbytes    | 248
tids      | {"(239,44)","(239,47)","(239,48)","(239,50)","(239,52)","(240,3)",...
-[ RECORD 2 ]---------------------------------------------------------------------
first_tid | (247,40)
nbytes    | 248
tids      | {"(247,40)","(247,41)","(247,44)","(247,45)","(247,46)","(248,2)",...
...
Note here that leave pages of the tree of TIDs actually contain small compressed lists of pointers to table rows rather than individual pointers.

Properties
Let's look at the properties of GIN access method (queries have already been provided).


 amname |     name      | pg_indexam_has_property 
--------+---------------+-------------------------
 gin    | can_order     | f
 gin    | can_unique    | f
 gin    | can_multi_col | t
 gin    | can_exclude   | f
Interestingly, GIN supports creation of multicolumn indexes. However, unlike for a regular B-tree, instead of compound keys, 
a multicolumn index will still store individual elements, and the column number will be indicated for each element.

The following index-layer properties are available:


     name      | pg_index_has_property 
---------------+-----------------------
 clusterable   | f
 index_scan    | f
 bitmap_scan   | t
 backward_scan | f
Note that returning results TID by TID (index scan) is not supported; only bitmap scan is possible.

Backward scan is not supported either: this feature is essential for index scan only, but not for bitmap scan.

And the following are column-layer properties:


        name        | pg_index_column_has_property 
--------------------+------------------------------
 asc                | f
 desc               | f
 nulls_first        | f
 nulls_last         | f
 orderable          | f
 distance_orderable | f
 returnable         | f
 search_array       | f
 search_nulls       | f
 
Nothing is available here: no sorting (which is clear), no use of the index as covering (since the document itself is not stored in the index), 
no manipulation of NULLs (since it does not make sense for elements of compound type).

Other data types
A few more extensions are available that add support of GIN for some data types.

"pg_trgm" enables us to determine "likeness" of words by comparing how many equal three-letter sequences (trigrams) are available. 
Two operator classes are added, "gist_trgm_ops" and "gin_trgm_ops", which support various operators, 
including comparison by means of LIKE and regular expressions. 
We can use this extension together with full-text search in order to suggest word options to fix typos.
"hstore" implements "key-value" storage. For this data type, operator classes for various access methods are available, including GIN. However, 
with the introduction of "jsonb" data type, there are no special reasons to use "hstore".
"intarray" extends the functionality of integer arrays. Index support includes GiST, as well as GIN ("gin__int_ops" operator class).
And these two extensions have already been mentioned above:

"btree_gin" adds GIN support for regular data types in order for them to be used in a multicolumn index along with compound types.
"jsquery" defines a language for JSON querying and an operator class for index support of this language. 
This extension is not included in a standard PostgreSQL delivery.














